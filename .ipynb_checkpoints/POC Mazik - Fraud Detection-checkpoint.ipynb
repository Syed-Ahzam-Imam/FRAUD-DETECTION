{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bOO2Gj6ZMsK",
   "metadata": {
    "id": "8bOO2Gj6ZMsK"
   },
   "source": [
    "## Importing Important Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b327ca75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:48:11.019908Z",
     "iopub.status.busy": "2023-10-24T17:48:11.018797Z",
     "iopub.status.idle": "2023-10-24T17:48:28.290008Z",
     "shell.execute_reply": "2023-10-24T17:48:28.287697Z"
    },
    "id": "b327ca75",
    "papermill": {
     "duration": 17.287325,
     "end_time": "2023-10-24T17:48:28.293313",
     "exception": false,
     "start_time": "2023-10-24T17:48:11.005988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings     # for supressing a warning when importing large files\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder , OrdinalEncoder , LabelEncoder\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d207692",
   "metadata": {
    "id": "7d207692",
    "papermill": {
     "duration": 0.010861,
     "end_time": "2023-10-24T17:48:28.314644",
     "exception": false,
     "start_time": "2023-10-24T17:48:28.303783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0396e528",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:48:28.338422Z",
     "iopub.status.busy": "2023-10-24T17:48:28.337547Z",
     "iopub.status.idle": "2023-10-24T17:48:32.162501Z",
     "shell.execute_reply": "2023-10-24T17:48:32.160699Z"
    },
    "id": "0396e528",
    "papermill": {
     "duration": 3.840008,
     "end_time": "2023-10-24T17:48:32.165155",
     "exception": false,
     "start_time": "2023-10-24T17:48:28.325147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Train=pd.read_csv(\"Dataset/Train-1542865627584.csv\")\n",
    "Train_Beneficiarydata=pd.read_csv(\"Dataset/Train_Beneficiarydata-1542865627584.csv\")\n",
    "Train_Inpatientdata=pd.read_csv(\"Dataset/Train_Inpatientdata-1542865627584.csv\")\n",
    "Train_Outpatientdata=pd.read_csv(\"Dataset/Train_Outpatientdata-1542865627584.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41548cab",
   "metadata": {
    "id": "41548cab",
    "papermill": {
     "duration": 0.0103,
     "end_time": "2023-10-24T17:48:32.186159",
     "exception": false,
     "start_time": "2023-10-24T17:48:32.175859",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Merging the  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a104549",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:48:32.209986Z",
     "iopub.status.busy": "2023-10-24T17:48:32.209541Z",
     "iopub.status.idle": "2023-10-24T17:48:34.802607Z",
     "shell.execute_reply": "2023-10-24T17:48:34.801053Z"
    },
    "id": "2a104549",
    "papermill": {
     "duration": 2.608813,
     "end_time": "2023-10-24T17:48:34.805655",
     "exception": false,
     "start_time": "2023-10-24T17:48:32.196842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Allpatientdata=pd.merge(Train_Outpatientdata,Train_Inpatientdata,\n",
    "                              left_on=['BeneID', 'ClaimID', 'ClaimStartDt', 'ClaimEndDt', 'Provider',\n",
    "       'InscClaimAmtReimbursed', 'AttendingPhysician', 'OperatingPhysician',\n",
    "       'OtherPhysician', 'ClmDiagnosisCode_1', 'ClmDiagnosisCode_2',\n",
    "       'ClmDiagnosisCode_3', 'ClmDiagnosisCode_4', 'ClmDiagnosisCode_5',\n",
    "       'ClmDiagnosisCode_6', 'ClmDiagnosisCode_7', 'ClmDiagnosisCode_8',\n",
    "       'ClmDiagnosisCode_9', 'ClmDiagnosisCode_10', 'ClmProcedureCode_1',\n",
    "       'ClmProcedureCode_2', 'ClmProcedureCode_3', 'ClmProcedureCode_4',\n",
    "       'ClmProcedureCode_5', 'ClmProcedureCode_6', 'DeductibleAmtPaid',\n",
    "       'ClmAdmitDiagnosisCode'],\n",
    "                              right_on=['BeneID', 'ClaimID', 'ClaimStartDt', 'ClaimEndDt', 'Provider',\n",
    "       'InscClaimAmtReimbursed', 'AttendingPhysician', 'OperatingPhysician',\n",
    "       'OtherPhysician', 'ClmDiagnosisCode_1', 'ClmDiagnosisCode_2',\n",
    "       'ClmDiagnosisCode_3', 'ClmDiagnosisCode_4', 'ClmDiagnosisCode_5',\n",
    "       'ClmDiagnosisCode_6', 'ClmDiagnosisCode_7', 'ClmDiagnosisCode_8',\n",
    "       'ClmDiagnosisCode_9', 'ClmDiagnosisCode_10', 'ClmProcedureCode_1',\n",
    "       'ClmProcedureCode_2', 'ClmProcedureCode_3', 'ClmProcedureCode_4',\n",
    "       'ClmProcedureCode_5', 'ClmProcedureCode_6', 'DeductibleAmtPaid',\n",
    "       'ClmAdmitDiagnosisCode']\n",
    "                              ,how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31bae6a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:48:34.827611Z",
     "iopub.status.busy": "2023-10-24T17:48:34.827232Z",
     "iopub.status.idle": "2023-10-24T17:48:35.571083Z",
     "shell.execute_reply": "2023-10-24T17:48:35.569486Z"
    },
    "id": "31bae6a0",
    "papermill": {
     "duration": 0.757896,
     "end_time": "2023-10-24T17:48:35.573756",
     "exception": false,
     "start_time": "2023-10-24T17:48:34.815860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Allpatientdata=pd.merge(Allpatientdata,Train_Beneficiarydata,on=\"BeneID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2246cd94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:48:35.598095Z",
     "iopub.status.busy": "2023-10-24T17:48:35.597391Z",
     "iopub.status.idle": "2023-10-24T17:48:37.908573Z",
     "shell.execute_reply": "2023-10-24T17:48:37.906376Z"
    },
    "id": "2246cd94",
    "papermill": {
     "duration": 2.326831,
     "end_time": "2023-10-24T17:48:37.911172",
     "exception": false,
     "start_time": "2023-10-24T17:48:35.584341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Allpatientdata=pd.merge(Allpatientdata,Train,on=\"Provider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82baecd4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "execution": {
     "iopub.execute_input": "2023-10-24T17:48:37.934408Z",
     "iopub.status.busy": "2023-10-24T17:48:37.932827Z",
     "iopub.status.idle": "2023-10-24T17:48:37.960938Z",
     "shell.execute_reply": "2023-10-24T17:48:37.959565Z"
    },
    "id": "82baecd4",
    "outputId": "7ed8064b-dd6d-44ea-9022-db3faf9c6403",
    "papermill": {
     "duration": 0.042426,
     "end_time": "2023-10-24T17:48:37.963794",
     "exception": false,
     "start_time": "2023-10-24T17:48:37.921368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Allpatientdata.head(5)\n",
    "Allpatientdata.to_csv(\"Combined Dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41012ca1",
   "metadata": {},
   "source": [
    "## Data Analysis and Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8631511b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-10-24T17:48:37.989109Z",
     "iopub.status.busy": "2023-10-24T17:48:37.987993Z",
     "iopub.status.idle": "2023-10-24T17:48:38.806155Z",
     "shell.execute_reply": "2023-10-24T17:48:38.804260Z"
    },
    "id": "8631511b",
    "outputId": "4245b6f9-ec8e-41ee-add7-83118c04a1c2",
    "papermill": {
     "duration": 0.833605,
     "end_time": "2023-10-24T17:48:38.808704",
     "exception": false,
     "start_time": "2023-10-24T17:48:37.975099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 558211 entries, 0 to 558210\n",
      "Data columns (total 55 columns):\n",
      " #   Column                           Non-Null Count   Dtype  \n",
      "---  ------                           --------------   -----  \n",
      " 0   BeneID                           558211 non-null  object \n",
      " 1   ClaimID                          558211 non-null  object \n",
      " 2   ClaimStartDt                     558211 non-null  object \n",
      " 3   ClaimEndDt                       558211 non-null  object \n",
      " 4   Provider                         558211 non-null  object \n",
      " 5   InscClaimAmtReimbursed           558211 non-null  int64  \n",
      " 6   AttendingPhysician               556703 non-null  object \n",
      " 7   OperatingPhysician               114447 non-null  object \n",
      " 8   OtherPhysician                   199736 non-null  object \n",
      " 9   ClmDiagnosisCode_1               547758 non-null  object \n",
      " 10  ClmDiagnosisCode_2               362605 non-null  object \n",
      " 11  ClmDiagnosisCode_3               243055 non-null  object \n",
      " 12  ClmDiagnosisCode_4               164536 non-null  object \n",
      " 13  ClmDiagnosisCode_5               111924 non-null  object \n",
      " 14  ClmDiagnosisCode_6               84392 non-null   object \n",
      " 15  ClmDiagnosisCode_7               66177 non-null   object \n",
      " 16  ClmDiagnosisCode_8               53444 non-null   object \n",
      " 17  ClmDiagnosisCode_9               41815 non-null   object \n",
      " 18  ClmDiagnosisCode_10              5010 non-null    object \n",
      " 19  ClmProcedureCode_1               23310 non-null   float64\n",
      " 20  ClmProcedureCode_2               5490 non-null    float64\n",
      " 21  ClmProcedureCode_3               969 non-null     float64\n",
      " 22  ClmProcedureCode_4               118 non-null     float64\n",
      " 23  ClmProcedureCode_5               9 non-null       float64\n",
      " 24  ClmProcedureCode_6               0 non-null       float64\n",
      " 25  DeductibleAmtPaid                557312 non-null  float64\n",
      " 26  ClmAdmitDiagnosisCode            145899 non-null  object \n",
      " 27  AdmissionDt                      40474 non-null   object \n",
      " 28  DischargeDt                      40474 non-null   object \n",
      " 29  DiagnosisGroupCode               40474 non-null   object \n",
      " 30  DOB                              558211 non-null  object \n",
      " 31  DOD                              4131 non-null    object \n",
      " 32  Gender                           558211 non-null  int64  \n",
      " 33  Race                             558211 non-null  int64  \n",
      " 34  RenalDiseaseIndicator            558211 non-null  object \n",
      " 35  State                            558211 non-null  int64  \n",
      " 36  County                           558211 non-null  int64  \n",
      " 37  NoOfMonths_PartACov              558211 non-null  int64  \n",
      " 38  NoOfMonths_PartBCov              558211 non-null  int64  \n",
      " 39  ChronicCond_Alzheimer            558211 non-null  int64  \n",
      " 40  ChronicCond_Heartfailure         558211 non-null  int64  \n",
      " 41  ChronicCond_KidneyDisease        558211 non-null  int64  \n",
      " 42  ChronicCond_Cancer               558211 non-null  int64  \n",
      " 43  ChronicCond_ObstrPulmonary       558211 non-null  int64  \n",
      " 44  ChronicCond_Depression           558211 non-null  int64  \n",
      " 45  ChronicCond_Diabetes             558211 non-null  int64  \n",
      " 46  ChronicCond_IschemicHeart        558211 non-null  int64  \n",
      " 47  ChronicCond_Osteoporasis         558211 non-null  int64  \n",
      " 48  ChronicCond_rheumatoidarthritis  558211 non-null  int64  \n",
      " 49  ChronicCond_stroke               558211 non-null  int64  \n",
      " 50  IPAnnualReimbursementAmt         558211 non-null  int64  \n",
      " 51  IPAnnualDeductibleAmt            558211 non-null  int64  \n",
      " 52  OPAnnualReimbursementAmt         558211 non-null  int64  \n",
      " 53  OPAnnualDeductibleAmt            558211 non-null  int64  \n",
      " 54  PotentialFraud                   558211 non-null  object \n",
      "dtypes: float64(7), int64(22), object(26)\n",
      "memory usage: 234.2+ MB\n"
     ]
    }
   ],
   "source": [
    "Allpatientdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f76a97d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "execution": {
     "iopub.execute_input": "2023-10-24T17:48:38.832219Z",
     "iopub.status.busy": "2023-10-24T17:48:38.830988Z",
     "iopub.status.idle": "2023-10-24T17:48:39.695924Z",
     "shell.execute_reply": "2023-10-24T17:48:39.694501Z"
    },
    "id": "f76a97d8",
    "outputId": "b28b21b6-fef7-4979-d513-74778dbe93b6",
    "papermill": {
     "duration": 0.878975,
     "end_time": "2023-10-24T17:48:39.698281",
     "exception": false,
     "start_time": "2023-10-24T17:48:38.819306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BeneID</th>\n",
       "      <th>ClaimID</th>\n",
       "      <th>ClaimStartDt</th>\n",
       "      <th>ClaimEndDt</th>\n",
       "      <th>Provider</th>\n",
       "      <th>InscClaimAmtReimbursed</th>\n",
       "      <th>AttendingPhysician</th>\n",
       "      <th>OperatingPhysician</th>\n",
       "      <th>OtherPhysician</th>\n",
       "      <th>ClmDiagnosisCode_1</th>\n",
       "      <th>...</th>\n",
       "      <th>ChronicCond_Diabetes</th>\n",
       "      <th>ChronicCond_IschemicHeart</th>\n",
       "      <th>ChronicCond_Osteoporasis</th>\n",
       "      <th>ChronicCond_rheumatoidarthritis</th>\n",
       "      <th>ChronicCond_stroke</th>\n",
       "      <th>IPAnnualReimbursementAmt</th>\n",
       "      <th>IPAnnualDeductibleAmt</th>\n",
       "      <th>OPAnnualReimbursementAmt</th>\n",
       "      <th>OPAnnualDeductibleAmt</th>\n",
       "      <th>PotentialFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558206</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558207</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558208</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558209</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558210</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>558211 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        BeneID  ClaimID  ClaimStartDt  ClaimEndDt  Provider  \\\n",
       "0        False    False         False       False     False   \n",
       "1        False    False         False       False     False   \n",
       "2        False    False         False       False     False   \n",
       "3        False    False         False       False     False   \n",
       "4        False    False         False       False     False   \n",
       "...        ...      ...           ...         ...       ...   \n",
       "558206   False    False         False       False     False   \n",
       "558207   False    False         False       False     False   \n",
       "558208   False    False         False       False     False   \n",
       "558209   False    False         False       False     False   \n",
       "558210   False    False         False       False     False   \n",
       "\n",
       "        InscClaimAmtReimbursed  AttendingPhysician  OperatingPhysician  \\\n",
       "0                        False               False                True   \n",
       "1                        False               False                True   \n",
       "2                        False               False                True   \n",
       "3                        False               False                True   \n",
       "4                        False               False                True   \n",
       "...                        ...                 ...                 ...   \n",
       "558206                   False               False               False   \n",
       "558207                   False               False                True   \n",
       "558208                   False               False                True   \n",
       "558209                   False               False                True   \n",
       "558210                   False               False                True   \n",
       "\n",
       "        OtherPhysician  ClmDiagnosisCode_1  ...  ChronicCond_Diabetes  \\\n",
       "0                 True               False  ...                 False   \n",
       "1                 True               False  ...                 False   \n",
       "2                 True               False  ...                 False   \n",
       "3                 True               False  ...                 False   \n",
       "4                 True               False  ...                 False   \n",
       "...                ...                 ...  ...                   ...   \n",
       "558206            True               False  ...                 False   \n",
       "558207            True               False  ...                 False   \n",
       "558208            True               False  ...                 False   \n",
       "558209            True               False  ...                 False   \n",
       "558210            True               False  ...                 False   \n",
       "\n",
       "        ChronicCond_IschemicHeart  ChronicCond_Osteoporasis  \\\n",
       "0                           False                     False   \n",
       "1                           False                     False   \n",
       "2                           False                     False   \n",
       "3                           False                     False   \n",
       "4                           False                     False   \n",
       "...                           ...                       ...   \n",
       "558206                      False                     False   \n",
       "558207                      False                     False   \n",
       "558208                      False                     False   \n",
       "558209                      False                     False   \n",
       "558210                      False                     False   \n",
       "\n",
       "        ChronicCond_rheumatoidarthritis  ChronicCond_stroke  \\\n",
       "0                                 False               False   \n",
       "1                                 False               False   \n",
       "2                                 False               False   \n",
       "3                                 False               False   \n",
       "4                                 False               False   \n",
       "...                                 ...                 ...   \n",
       "558206                            False               False   \n",
       "558207                            False               False   \n",
       "558208                            False               False   \n",
       "558209                            False               False   \n",
       "558210                            False               False   \n",
       "\n",
       "        IPAnnualReimbursementAmt  IPAnnualDeductibleAmt  \\\n",
       "0                          False                  False   \n",
       "1                          False                  False   \n",
       "2                          False                  False   \n",
       "3                          False                  False   \n",
       "4                          False                  False   \n",
       "...                          ...                    ...   \n",
       "558206                     False                  False   \n",
       "558207                     False                  False   \n",
       "558208                     False                  False   \n",
       "558209                     False                  False   \n",
       "558210                     False                  False   \n",
       "\n",
       "        OPAnnualReimbursementAmt  OPAnnualDeductibleAmt  PotentialFraud  \n",
       "0                          False                  False           False  \n",
       "1                          False                  False           False  \n",
       "2                          False                  False           False  \n",
       "3                          False                  False           False  \n",
       "4                          False                  False           False  \n",
       "...                          ...                    ...             ...  \n",
       "558206                     False                  False           False  \n",
       "558207                     False                  False           False  \n",
       "558208                     False                  False           False  \n",
       "558209                     False                  False           False  \n",
       "558210                     False                  False           False  \n",
       "\n",
       "[558211 rows x 55 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Allpatientdata.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01293273",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-10-24T17:48:39.721847Z",
     "iopub.status.busy": "2023-10-24T17:48:39.721510Z",
     "iopub.status.idle": "2023-10-24T17:48:39.962675Z",
     "shell.execute_reply": "2023-10-24T17:48:39.961814Z"
    },
    "id": "01293273",
    "outputId": "4bc6790f-0d2c-47d5-ed98-15c165bdbe8a",
    "papermill": {
     "duration": 0.25525,
     "end_time": "2023-10-24T17:48:39.964616",
     "exception": false,
     "start_time": "2023-10-24T17:48:39.709366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61164\n",
      "15884\n",
      "85258\n",
      "63872\n"
     ]
    }
   ],
   "source": [
    "a=(Allpatientdata[\"AttendingPhysician\"]==Allpatientdata[\"OperatingPhysician\"])\n",
    "b=(Allpatientdata[\"OperatingPhysician\"]==Allpatientdata[\"OtherPhysician\"])\n",
    "c=(Allpatientdata[\"AttendingPhysician\"]==Allpatientdata[\"OtherPhysician\"])\n",
    "\n",
    "print(a.sum())\n",
    "print(b.sum())\n",
    "print(c.sum())\n",
    "print( (a+b).sum() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbb43d07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:48:40.047646Z",
     "iopub.status.busy": "2023-10-24T17:48:40.047076Z",
     "iopub.status.idle": "2023-10-24T17:48:52.362561Z",
     "shell.execute_reply": "2023-10-24T17:48:52.361410Z"
    },
    "id": "dbb43d07",
    "papermill": {
     "duration": 12.33216,
     "end_time": "2023-10-24T17:48:52.366057",
     "exception": false,
     "start_time": "2023-10-24T17:48:40.033897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def physician_same(row):\n",
    "    atten_oper=row[\"AttendingPhysician\"]==row[\"OperatingPhysician\"]\n",
    "    oper_other=row[\"OperatingPhysician\"]==row[\"OtherPhysician\"]\n",
    "    atten_other=row[\"AttendingPhysician\"]==row[\"OtherPhysician\"]\n",
    "    if atten_oper==True and oper_other==True:# atten = oper = other\n",
    "        return 0\n",
    "    elif atten_oper==True and oper_other==False:# atten = oper != other\n",
    "        return 1\n",
    "    elif atten_oper==False and oper_other==True:# atten != oper = other\n",
    "        return 2\n",
    "    else:# atten != oper != other\n",
    "        return 3\n",
    "\n",
    "phy_same=Allpatientdata.apply(physician_same,axis=1)\n",
    "Allpatientdata[\"phy_same\"]=phy_same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f04ff63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:48:52.391613Z",
     "iopub.status.busy": "2023-10-24T17:48:52.391222Z",
     "iopub.status.idle": "2023-10-24T17:49:01.431352Z",
     "shell.execute_reply": "2023-10-24T17:49:01.430083Z"
    },
    "id": "0f04ff63",
    "papermill": {
     "duration": 9.055693,
     "end_time": "2023-10-24T17:49:01.434097",
     "exception": false,
     "start_time": "2023-10-24T17:48:52.378404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def physician_count(row,list_count):\n",
    "    count=0\n",
    "    for col in list_count:\n",
    "        if pd.isnull(row[col]):\n",
    "            continue\n",
    "        else:\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "list_count=[\"AttendingPhysician\",\"OperatingPhysician\",\"OtherPhysician\"]\n",
    "phy_count=Allpatientdata.apply(physician_count,axis=1,args=(list_count,))\n",
    "Allpatientdata[\"phy_count\"]=phy_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "09939548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            BeneID    ClaimID ClaimStartDt  ClaimEndDt  Provider  \\\n",
      "0        BENE11002  CLM624349   2009-10-11  2009-10-11  PRV56011   \n",
      "1        BENE11004  CLM121801   2009-01-06  2009-01-06  PRV56011   \n",
      "2        BENE11004  CLM150998   2009-01-22  2009-01-22  PRV56011   \n",
      "3        BENE11004  CLM173224   2009-02-03  2009-02-03  PRV56011   \n",
      "4        BENE11004  CLM224741   2009-03-03  2009-03-03  PRV56011   \n",
      "...            ...        ...          ...         ...       ...   \n",
      "558206   BENE57978   CLM37910   2009-02-11  2009-02-16  PRV53511   \n",
      "558207   BENE80245   CLM79653   2009-12-15  2009-12-18  PRV51711   \n",
      "558208   BENE85177   CLM77745   2009-11-29  2009-12-07  PRV54503   \n",
      "558209  BENE105577   CLM53155   2009-05-28  2009-05-31  PRV51239   \n",
      "558210  BENE118935   CLM46237   2009-04-09  2009-04-17  PRV53516   \n",
      "\n",
      "        InscClaimAmtReimbursed AttendingPhysician OperatingPhysician  \\\n",
      "0                           30          PHY326117                NaN   \n",
      "1                           40          PHY334319                NaN   \n",
      "2                          200          PHY403831                NaN   \n",
      "3                           20          PHY339887                NaN   \n",
      "4                           40          PHY345721                NaN   \n",
      "...                        ...                ...                ...   \n",
      "558206                       0          PHY378514          PHY426843   \n",
      "558207                    4000          PHY345109                NaN   \n",
      "558208                    8000          PHY418343                NaN   \n",
      "558209                    5000          PHY382916                NaN   \n",
      "558210                    3000          PHY362672                NaN   \n",
      "\n",
      "       OtherPhysician ClmDiagnosisCode_1  ... ChronicCond_rheumatoidarthritis  \\\n",
      "0                 NaN              78943  ...                               2   \n",
      "1                 NaN              71988  ...                               1   \n",
      "2                 NaN              82382  ...                               1   \n",
      "3                 NaN              20381  ...                               1   \n",
      "4                 NaN              V6546  ...                               1   \n",
      "...               ...                ...  ...                             ...   \n",
      "558206            NaN              40491  ...                               1   \n",
      "558207            NaN               2948  ...                               2   \n",
      "558208            NaN                515  ...                               2   \n",
      "558209            NaN              27651  ...                               2   \n",
      "558210            NaN              49322  ...                               2   \n",
      "\n",
      "       ChronicCond_stroke IPAnnualReimbursementAmt IPAnnualDeductibleAmt  \\\n",
      "0                       2                        0                     0   \n",
      "1                       2                        0                     0   \n",
      "2                       2                        0                     0   \n",
      "3                       2                        0                     0   \n",
      "4                       2                        0                     0   \n",
      "...                   ...                      ...                   ...   \n",
      "558206                  2                     2200                  2136   \n",
      "558207                  2                     4000                  1068   \n",
      "558208                  2                     8000                  1068   \n",
      "558209                  2                     5000                  1068   \n",
      "558210                  2                    18800                  3204   \n",
      "\n",
      "       OPAnnualReimbursementAmt OPAnnualDeductibleAmt PotentialFraud phy_same  \\\n",
      "0                            30                    50            Yes        3   \n",
      "1                          1810                   760            Yes        3   \n",
      "2                          1810                   760            Yes        3   \n",
      "3                          1810                   760            Yes        3   \n",
      "4                          1810                   760            Yes        3   \n",
      "...                         ...                   ...            ...      ...   \n",
      "558206                       30                   100             No        3   \n",
      "558207                        0                     0             No        3   \n",
      "558208                        0                     0             No        3   \n",
      "558209                        0                     0             No        3   \n",
      "558210                        0                     0             No        3   \n",
      "\n",
      "       phy_count  period  \n",
      "0              1       0  \n",
      "1              1       0  \n",
      "2              1       0  \n",
      "3              1       0  \n",
      "4              1       0  \n",
      "...          ...     ...  \n",
      "558206         2       5  \n",
      "558207         1       3  \n",
      "558208         1       8  \n",
      "558209         1       3  \n",
      "558210         1       8  \n",
      "\n",
      "[558211 rows x 58 columns]\n"
     ]
    }
   ],
   "source": [
    "print(Allpatientdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a4c73d",
   "metadata": {
    "id": "d9a4c73d",
    "papermill": {
     "duration": 0.01095,
     "end_time": "2023-10-24T17:49:01.456308",
     "exception": false,
     "start_time": "2023-10-24T17:49:01.445358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ClaimStartDt , ClaimEndDt cols preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e9ee912",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:01.480985Z",
     "iopub.status.busy": "2023-10-24T17:49:01.480586Z",
     "iopub.status.idle": "2023-10-24T17:49:01.659184Z",
     "shell.execute_reply": "2023-10-24T17:49:01.657955Z"
    },
    "id": "3e9ee912",
    "papermill": {
     "duration": 0.19407,
     "end_time": "2023-10-24T17:49:01.661805",
     "exception": false,
     "start_time": "2023-10-24T17:49:01.467735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "startdate= pd.to_datetime( Allpatientdata[\"ClaimStartDt\"] )\n",
    "enddate= pd.to_datetime( Allpatientdata[\"ClaimEndDt\"] )\n",
    "\n",
    "period = ( enddate - startdate).dt.days\n",
    "Allpatientdata[\"period\"] = period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "300668ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:01.686823Z",
     "iopub.status.busy": "2023-10-24T17:49:01.686364Z",
     "iopub.status.idle": "2023-10-24T17:49:01.707862Z",
     "shell.execute_reply": "2023-10-24T17:49:01.706813Z"
    },
    "id": "300668ba",
    "outputId": "942b645a-05d3-444c-fcfa-45af131c38ae",
    "papermill": {
     "duration": 0.037218,
     "end_time": "2023-10-24T17:49:01.710509",
     "exception": false,
     "start_time": "2023-10-24T17:49:01.673291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BeneID</th>\n",
       "      <th>ClaimID</th>\n",
       "      <th>ClaimStartDt</th>\n",
       "      <th>ClaimEndDt</th>\n",
       "      <th>Provider</th>\n",
       "      <th>InscClaimAmtReimbursed</th>\n",
       "      <th>AttendingPhysician</th>\n",
       "      <th>OperatingPhysician</th>\n",
       "      <th>OtherPhysician</th>\n",
       "      <th>ClmDiagnosisCode_1</th>\n",
       "      <th>...</th>\n",
       "      <th>ChronicCond_rheumatoidarthritis</th>\n",
       "      <th>ChronicCond_stroke</th>\n",
       "      <th>IPAnnualReimbursementAmt</th>\n",
       "      <th>IPAnnualDeductibleAmt</th>\n",
       "      <th>OPAnnualReimbursementAmt</th>\n",
       "      <th>OPAnnualDeductibleAmt</th>\n",
       "      <th>PotentialFraud</th>\n",
       "      <th>phy_same</th>\n",
       "      <th>phy_count</th>\n",
       "      <th>period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BENE11002</td>\n",
       "      <td>CLM624349</td>\n",
       "      <td>2009-10-11</td>\n",
       "      <td>2009-10-11</td>\n",
       "      <td>PRV56011</td>\n",
       "      <td>30</td>\n",
       "      <td>PHY326117</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78943</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BENE11004</td>\n",
       "      <td>CLM121801</td>\n",
       "      <td>2009-01-06</td>\n",
       "      <td>2009-01-06</td>\n",
       "      <td>PRV56011</td>\n",
       "      <td>40</td>\n",
       "      <td>PHY334319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71988</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1810</td>\n",
       "      <td>760</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BENE11004</td>\n",
       "      <td>CLM150998</td>\n",
       "      <td>2009-01-22</td>\n",
       "      <td>2009-01-22</td>\n",
       "      <td>PRV56011</td>\n",
       "      <td>200</td>\n",
       "      <td>PHY403831</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82382</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1810</td>\n",
       "      <td>760</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BENE11004</td>\n",
       "      <td>CLM173224</td>\n",
       "      <td>2009-02-03</td>\n",
       "      <td>2009-02-03</td>\n",
       "      <td>PRV56011</td>\n",
       "      <td>20</td>\n",
       "      <td>PHY339887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20381</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1810</td>\n",
       "      <td>760</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BENE11004</td>\n",
       "      <td>CLM224741</td>\n",
       "      <td>2009-03-03</td>\n",
       "      <td>2009-03-03</td>\n",
       "      <td>PRV56011</td>\n",
       "      <td>40</td>\n",
       "      <td>PHY345721</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V6546</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1810</td>\n",
       "      <td>760</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      BeneID    ClaimID ClaimStartDt  ClaimEndDt  Provider  \\\n",
       "0  BENE11002  CLM624349   2009-10-11  2009-10-11  PRV56011   \n",
       "1  BENE11004  CLM121801   2009-01-06  2009-01-06  PRV56011   \n",
       "2  BENE11004  CLM150998   2009-01-22  2009-01-22  PRV56011   \n",
       "3  BENE11004  CLM173224   2009-02-03  2009-02-03  PRV56011   \n",
       "4  BENE11004  CLM224741   2009-03-03  2009-03-03  PRV56011   \n",
       "\n",
       "   InscClaimAmtReimbursed AttendingPhysician OperatingPhysician  \\\n",
       "0                      30          PHY326117                NaN   \n",
       "1                      40          PHY334319                NaN   \n",
       "2                     200          PHY403831                NaN   \n",
       "3                      20          PHY339887                NaN   \n",
       "4                      40          PHY345721                NaN   \n",
       "\n",
       "  OtherPhysician ClmDiagnosisCode_1  ... ChronicCond_rheumatoidarthritis  \\\n",
       "0            NaN              78943  ...                               2   \n",
       "1            NaN              71988  ...                               1   \n",
       "2            NaN              82382  ...                               1   \n",
       "3            NaN              20381  ...                               1   \n",
       "4            NaN              V6546  ...                               1   \n",
       "\n",
       "  ChronicCond_stroke IPAnnualReimbursementAmt IPAnnualDeductibleAmt  \\\n",
       "0                  2                        0                     0   \n",
       "1                  2                        0                     0   \n",
       "2                  2                        0                     0   \n",
       "3                  2                        0                     0   \n",
       "4                  2                        0                     0   \n",
       "\n",
       "  OPAnnualReimbursementAmt OPAnnualDeductibleAmt PotentialFraud phy_same  \\\n",
       "0                       30                    50            Yes        3   \n",
       "1                     1810                   760            Yes        3   \n",
       "2                     1810                   760            Yes        3   \n",
       "3                     1810                   760            Yes        3   \n",
       "4                     1810                   760            Yes        3   \n",
       "\n",
       "  phy_count  period  \n",
       "0         1       0  \n",
       "1         1       0  \n",
       "2         1       0  \n",
       "3         1       0  \n",
       "4         1       0  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Allpatientdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adaf67e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:01.735115Z",
     "iopub.status.busy": "2023-10-24T17:49:01.734730Z",
     "iopub.status.idle": "2023-10-24T17:49:02.386332Z",
     "shell.execute_reply": "2023-10-24T17:49:02.384717Z"
    },
    "id": "adaf67e6",
    "papermill": {
     "duration": 0.666791,
     "end_time": "2023-10-24T17:49:02.388878",
     "exception": false,
     "start_time": "2023-10-24T17:49:01.722087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "copy = Allpatientdata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b3ec35f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:02.416172Z",
     "iopub.status.busy": "2023-10-24T17:49:02.415762Z",
     "iopub.status.idle": "2023-10-24T17:49:02.485635Z",
     "shell.execute_reply": "2023-10-24T17:49:02.484356Z"
    },
    "id": "7b3ec35f",
    "papermill": {
     "duration": 0.087607,
     "end_time": "2023-10-24T17:49:02.488451",
     "exception": false,
     "start_time": "2023-10-24T17:49:02.400844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cronic_cols_names=copy.columns[ copy.columns.str.startswith(\"ChronicCond\") ]\n",
    "cronic_cols=copy[   cronic_cols_names   ]\n",
    "cronic=cronic_cols.replace({2:0})\n",
    "copy[   cronic_cols_names   ]=cronic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "defaa902",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:02.514894Z",
     "iopub.status.busy": "2023-10-24T17:49:02.514557Z",
     "iopub.status.idle": "2023-10-24T17:49:02.654657Z",
     "shell.execute_reply": "2023-10-24T17:49:02.653029Z"
    },
    "id": "defaa902",
    "papermill": {
     "duration": 0.15749,
     "end_time": "2023-10-24T17:49:02.657474",
     "exception": false,
     "start_time": "2023-10-24T17:49:02.499984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "copy[\"PotentialFraud\"]=copy[\"PotentialFraud\"].replace({\"Yes\":1,\"No\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "660d63a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:02.682886Z",
     "iopub.status.busy": "2023-10-24T17:49:02.682532Z",
     "iopub.status.idle": "2023-10-24T17:49:02.692821Z",
     "shell.execute_reply": "2023-10-24T17:49:02.691794Z"
    },
    "id": "660d63a2",
    "papermill": {
     "duration": 0.025775,
     "end_time": "2023-10-24T17:49:02.695011",
     "exception": false,
     "start_time": "2023-10-24T17:49:02.669236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "copy[\"Gender\"]=copy[\"Gender\"].replace({2:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bdb6b9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:02.720032Z",
     "iopub.status.busy": "2023-10-24T17:49:02.719692Z",
     "iopub.status.idle": "2023-10-24T17:49:02.818370Z",
     "shell.execute_reply": "2023-10-24T17:49:02.817020Z"
    },
    "id": "2bdb6b9e",
    "papermill": {
     "duration": 0.114251,
     "end_time": "2023-10-24T17:49:02.821392",
     "exception": false,
     "start_time": "2023-10-24T17:49:02.707141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "startadmt= pd.to_datetime( copy[\"AdmissionDt\"] )\n",
    "enddatadmt= pd.to_datetime( copy[\"DischargeDt\"] )\n",
    "\n",
    "periodadmt = ( enddatadmt - startadmt).dt.days\n",
    "copy[\"periodadmt\"] = periodadmt\n",
    "copy[\"periodadmt\"]=copy[\"periodadmt\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b8381eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:02.847283Z",
     "iopub.status.busy": "2023-10-24T17:49:02.846898Z",
     "iopub.status.idle": "2023-10-24T17:49:02.897681Z",
     "shell.execute_reply": "2023-10-24T17:49:02.896612Z"
    },
    "id": "9b8381eb",
    "papermill": {
     "duration": 0.066777,
     "end_time": "2023-10-24T17:49:02.900226",
     "exception": false,
     "start_time": "2023-10-24T17:49:02.833449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "copy[\"RenalDiseaseIndicator\"]=copy[\"RenalDiseaseIndicator\"].replace({\"Y\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fceed6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:02.925065Z",
     "iopub.status.busy": "2023-10-24T17:49:02.924694Z",
     "iopub.status.idle": "2023-10-24T17:49:03.222825Z",
     "shell.execute_reply": "2023-10-24T17:49:03.221437Z"
    },
    "id": "8fceed6a",
    "papermill": {
     "duration": 0.313588,
     "end_time": "2023-10-24T17:49:03.225431",
     "exception": false,
     "start_time": "2023-10-24T17:49:02.911843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "birthdate=pd.to_datetime(copy[\"DOB\"])\n",
    "enddate=pd.to_datetime(copy[\"DOD\"])\n",
    "\n",
    "# check whether the patient dead or alife\n",
    "def alife_function(value):\n",
    "    if value==True:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "alife = pd.isna(enddate).apply(alife_function)\n",
    "\n",
    "\n",
    "# get the age of patient\n",
    "max_date=enddate.dropna().max()\n",
    "enddate[pd.isna(enddate)]=max_date\n",
    "period=(((enddate-birthdate).dt.days/356).astype(int))\n",
    "\n",
    "copy[\"age\"]=period\n",
    "copy[\"alife\"]=alife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66a5920b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:03.251616Z",
     "iopub.status.busy": "2023-10-24T17:49:03.251209Z",
     "iopub.status.idle": "2023-10-24T17:49:03.272549Z",
     "shell.execute_reply": "2023-10-24T17:49:03.271269Z"
    },
    "id": "66a5920b",
    "outputId": "819c655a-ec10-498b-f862-54d93907a181",
    "papermill": {
     "duration": 0.037479,
     "end_time": "2023-10-24T17:49:03.275063",
     "exception": false,
     "start_time": "2023-10-24T17:49:03.237584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BeneID</th>\n",
       "      <th>ClaimID</th>\n",
       "      <th>ClaimStartDt</th>\n",
       "      <th>ClaimEndDt</th>\n",
       "      <th>Provider</th>\n",
       "      <th>InscClaimAmtReimbursed</th>\n",
       "      <th>AttendingPhysician</th>\n",
       "      <th>OperatingPhysician</th>\n",
       "      <th>OtherPhysician</th>\n",
       "      <th>ClmDiagnosisCode_1</th>\n",
       "      <th>...</th>\n",
       "      <th>IPAnnualDeductibleAmt</th>\n",
       "      <th>OPAnnualReimbursementAmt</th>\n",
       "      <th>OPAnnualDeductibleAmt</th>\n",
       "      <th>PotentialFraud</th>\n",
       "      <th>phy_same</th>\n",
       "      <th>phy_count</th>\n",
       "      <th>period</th>\n",
       "      <th>periodadmt</th>\n",
       "      <th>age</th>\n",
       "      <th>alife</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BENE11002</td>\n",
       "      <td>CLM624349</td>\n",
       "      <td>2009-10-11</td>\n",
       "      <td>2009-10-11</td>\n",
       "      <td>PRV56011</td>\n",
       "      <td>30</td>\n",
       "      <td>PHY326117</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78943</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BENE11004</td>\n",
       "      <td>CLM121801</td>\n",
       "      <td>2009-01-06</td>\n",
       "      <td>2009-01-06</td>\n",
       "      <td>PRV56011</td>\n",
       "      <td>40</td>\n",
       "      <td>PHY334319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71988</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1810</td>\n",
       "      <td>760</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BENE11004</td>\n",
       "      <td>CLM150998</td>\n",
       "      <td>2009-01-22</td>\n",
       "      <td>2009-01-22</td>\n",
       "      <td>PRV56011</td>\n",
       "      <td>200</td>\n",
       "      <td>PHY403831</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82382</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1810</td>\n",
       "      <td>760</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BENE11004</td>\n",
       "      <td>CLM173224</td>\n",
       "      <td>2009-02-03</td>\n",
       "      <td>2009-02-03</td>\n",
       "      <td>PRV56011</td>\n",
       "      <td>20</td>\n",
       "      <td>PHY339887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20381</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1810</td>\n",
       "      <td>760</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BENE11004</td>\n",
       "      <td>CLM224741</td>\n",
       "      <td>2009-03-03</td>\n",
       "      <td>2009-03-03</td>\n",
       "      <td>PRV56011</td>\n",
       "      <td>40</td>\n",
       "      <td>PHY345721</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V6546</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1810</td>\n",
       "      <td>760</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      BeneID    ClaimID ClaimStartDt  ClaimEndDt  Provider  \\\n",
       "0  BENE11002  CLM624349   2009-10-11  2009-10-11  PRV56011   \n",
       "1  BENE11004  CLM121801   2009-01-06  2009-01-06  PRV56011   \n",
       "2  BENE11004  CLM150998   2009-01-22  2009-01-22  PRV56011   \n",
       "3  BENE11004  CLM173224   2009-02-03  2009-02-03  PRV56011   \n",
       "4  BENE11004  CLM224741   2009-03-03  2009-03-03  PRV56011   \n",
       "\n",
       "   InscClaimAmtReimbursed AttendingPhysician OperatingPhysician  \\\n",
       "0                      30          PHY326117                NaN   \n",
       "1                      40          PHY334319                NaN   \n",
       "2                     200          PHY403831                NaN   \n",
       "3                      20          PHY339887                NaN   \n",
       "4                      40          PHY345721                NaN   \n",
       "\n",
       "  OtherPhysician ClmDiagnosisCode_1  ... IPAnnualDeductibleAmt  \\\n",
       "0            NaN              78943  ...                     0   \n",
       "1            NaN              71988  ...                     0   \n",
       "2            NaN              82382  ...                     0   \n",
       "3            NaN              20381  ...                     0   \n",
       "4            NaN              V6546  ...                     0   \n",
       "\n",
       "  OPAnnualReimbursementAmt OPAnnualDeductibleAmt PotentialFraud phy_same  \\\n",
       "0                       30                    50              1        3   \n",
       "1                     1810                   760              1        3   \n",
       "2                     1810                   760              1        3   \n",
       "3                     1810                   760              1        3   \n",
       "4                     1810                   760              1        3   \n",
       "\n",
       "  phy_count period periodadmt age  alife  \n",
       "0         1      0        0.0  75      1  \n",
       "1         1      0        0.0  89      1  \n",
       "2         1      0        0.0  89      1  \n",
       "3         1      0        0.0  89      1  \n",
       "4         1      0        0.0  89      1  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "004493ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:03.300862Z",
     "iopub.status.busy": "2023-10-24T17:49:03.300457Z",
     "iopub.status.idle": "2023-10-24T17:49:03.309240Z",
     "shell.execute_reply": "2023-10-24T17:49:03.308038Z"
    },
    "id": "004493ac",
    "papermill": {
     "duration": 0.024764,
     "end_time": "2023-10-24T17:49:03.311817",
     "exception": false,
     "start_time": "2023-10-24T17:49:03.287053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def groupby(df,by,vars_to_group,methods,col_ident,as_index=True,agg=False):\n",
    "    if agg:\n",
    "        grouped=df.groupby(by=by,as_index=as_index)[vars_to_group].agg(methods)\n",
    "        cols=['_'.join(col) for col in grouped.columns.values]\n",
    "        cols=[col_ident+\"_\"+col for col in cols]\n",
    "        grouped.columns=cols\n",
    "        return grouped\n",
    "\n",
    "    else:\n",
    "        concat=df.groupby(by=by,as_index=as_index)[vars_to_group].transform(methods[0])\n",
    "        cols=[ col_ident+\"_\"+col+\"_\"+methods[0] for col in concat.columns ]\n",
    "        concat.columns=cols\n",
    "\n",
    "        for method in methods[1:]:\n",
    "            grouped=df.groupby(by=by,as_index=as_index)[vars_to_group].transform(method)\n",
    "            cols=[col_ident+\"_\"+col+\"_\"+method for col in grouped.columns]\n",
    "            grouped.columns=cols\n",
    "            concat=pd.concat([concat,grouped],axis=1)\n",
    "\n",
    "        return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fa24130",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:03.337562Z",
     "iopub.status.busy": "2023-10-24T17:49:03.337132Z",
     "iopub.status.idle": "2023-10-24T17:49:03.341707Z",
     "shell.execute_reply": "2023-10-24T17:49:03.340796Z"
    },
    "id": "2fa24130",
    "papermill": {
     "duration": 0.020421,
     "end_time": "2023-10-24T17:49:03.344305",
     "exception": false,
     "start_time": "2023-10-24T17:49:03.323884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "money_cols=[\"InscClaimAmtReimbursed\",\"DeductibleAmtPaid\",\"NoOfMonths_PartACov\",\"NoOfMonths_PartBCov\",\n",
    "           \"IPAnnualReimbursementAmt\",\"IPAnnualDeductibleAmt\",\"OPAnnualReimbursementAmt\",\"OPAnnualDeductibleAmt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a96673a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:03.370082Z",
     "iopub.status.busy": "2023-10-24T17:49:03.369599Z",
     "iopub.status.idle": "2023-10-24T17:49:03.682378Z",
     "shell.execute_reply": "2023-10-24T17:49:03.681156Z"
    },
    "id": "2a96673a",
    "papermill": {
     "duration": 0.328631,
     "end_time": "2023-10-24T17:49:03.685086",
     "exception": false,
     "start_time": "2023-10-24T17:49:03.356455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "provider_money=groupby(copy,[\"Provider\"],money_cols,[\"mean\",\"std\"],\"provider\",\n",
    "                       True,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af8ab9e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:03.711427Z",
     "iopub.status.busy": "2023-10-24T17:49:03.710985Z",
     "iopub.status.idle": "2023-10-24T17:49:05.005681Z",
     "shell.execute_reply": "2023-10-24T17:49:05.003868Z"
    },
    "id": "af8ab9e5",
    "papermill": {
     "duration": 1.311456,
     "end_time": "2023-10-24T17:49:05.008610",
     "exception": false,
     "start_time": "2023-10-24T17:49:03.697154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "banel_money=groupby(copy,[\"BeneID\"],money_cols,[\"mean\",\"std\"],\"banel\",\n",
    "                       True,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84056aa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:05.035470Z",
     "iopub.status.busy": "2023-10-24T17:49:05.035077Z",
     "iopub.status.idle": "2023-10-24T17:49:05.640994Z",
     "shell.execute_reply": "2023-10-24T17:49:05.638660Z"
    },
    "id": "84056aa7",
    "papermill": {
     "duration": 0.622298,
     "end_time": "2023-10-24T17:49:05.643611",
     "exception": false,
     "start_time": "2023-10-24T17:49:05.021313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "diag1_money=groupby(copy,[\"ClmDiagnosisCode_1\"],money_cols,[\"mean\",\"std\"],\"diag1\",\n",
    "                       True,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "416c7d34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:05.669797Z",
     "iopub.status.busy": "2023-10-24T17:49:05.669462Z",
     "iopub.status.idle": "2023-10-24T17:49:05.694716Z",
     "shell.execute_reply": "2023-10-24T17:49:05.692851Z"
    },
    "id": "416c7d34",
    "papermill": {
     "duration": 0.041121,
     "end_time": "2023-10-24T17:49:05.697550",
     "exception": false,
     "start_time": "2023-10-24T17:49:05.656429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_cols_names=[\"phy_same\",\"phy_count\",\"period\",\"periodadmt\",\"age\",\"alife\",\"Provider\",\"PotentialFraud\"]\n",
    "selected_cols=copy[selected_cols_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10a29e76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:05.724082Z",
     "iopub.status.busy": "2023-10-24T17:49:05.723566Z",
     "iopub.status.idle": "2023-10-24T17:49:05.813422Z",
     "shell.execute_reply": "2023-10-24T17:49:05.811810Z"
    },
    "id": "10a29e76",
    "papermill": {
     "duration": 0.106169,
     "end_time": "2023-10-24T17:49:05.815986",
     "exception": false,
     "start_time": "2023-10-24T17:49:05.709817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data=pd.concat([selected_cols,provider_money,banel_money,diag1_money],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2731229",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:05.841867Z",
     "iopub.status.busy": "2023-10-24T17:49:05.841492Z",
     "iopub.status.idle": "2023-10-24T17:49:06.040705Z",
     "shell.execute_reply": "2023-10-24T17:49:06.039109Z"
    },
    "id": "e2731229",
    "papermill": {
     "duration": 0.215302,
     "end_time": "2023-10-24T17:49:06.043262",
     "exception": false,
     "start_time": "2023-10-24T17:49:05.827960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped=data.groupby(by=[\"Provider\",\"PotentialFraud\"]).agg(\"mean\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6787bda4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:06.068791Z",
     "iopub.status.busy": "2023-10-24T17:49:06.068418Z",
     "iopub.status.idle": "2023-10-24T17:49:06.093550Z",
     "shell.execute_reply": "2023-10-24T17:49:06.092523Z"
    },
    "id": "6787bda4",
    "outputId": "03e906bc-8fb0-4db3-d599-30624461e4ca",
    "papermill": {
     "duration": 0.040631,
     "end_time": "2023-10-24T17:49:06.095762",
     "exception": false,
     "start_time": "2023-10-24T17:49:06.055131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Provider</th>\n",
       "      <th>PotentialFraud</th>\n",
       "      <th>phy_same</th>\n",
       "      <th>phy_count</th>\n",
       "      <th>period</th>\n",
       "      <th>periodadmt</th>\n",
       "      <th>age</th>\n",
       "      <th>alife</th>\n",
       "      <th>provider_InscClaimAmtReimbursed_mean</th>\n",
       "      <th>provider_DeductibleAmtPaid_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>diag1_OPAnnualReimbursementAmt_mean</th>\n",
       "      <th>diag1_OPAnnualDeductibleAmt_mean</th>\n",
       "      <th>diag1_InscClaimAmtReimbursed_std</th>\n",
       "      <th>diag1_DeductibleAmtPaid_std</th>\n",
       "      <th>diag1_NoOfMonths_PartACov_std</th>\n",
       "      <th>diag1_NoOfMonths_PartBCov_std</th>\n",
       "      <th>diag1_IPAnnualReimbursementAmt_std</th>\n",
       "      <th>diag1_IPAnnualDeductibleAmt_std</th>\n",
       "      <th>diag1_OPAnnualReimbursementAmt_std</th>\n",
       "      <th>diag1_OPAnnualDeductibleAmt_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PRV51001</td>\n",
       "      <td>0</td>\n",
       "      <td>2.960000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.440000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>80.240000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4185.600000</td>\n",
       "      <td>213.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>1804.331094</td>\n",
       "      <td>543.045084</td>\n",
       "      <td>3482.066310</td>\n",
       "      <td>161.353027</td>\n",
       "      <td>0.569945</td>\n",
       "      <td>0.424192</td>\n",
       "      <td>12941.552352</td>\n",
       "      <td>1205.297144</td>\n",
       "      <td>2450.076771</td>\n",
       "      <td>661.506672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PRV51003</td>\n",
       "      <td>1</td>\n",
       "      <td>2.439394</td>\n",
       "      <td>1.530303</td>\n",
       "      <td>3.674242</td>\n",
       "      <td>2.424242</td>\n",
       "      <td>71.371212</td>\n",
       "      <td>0.992424</td>\n",
       "      <td>4588.409091</td>\n",
       "      <td>502.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>2422.458599</td>\n",
       "      <td>676.313985</td>\n",
       "      <td>4017.871066</td>\n",
       "      <td>260.257069</td>\n",
       "      <td>0.726572</td>\n",
       "      <td>0.653285</td>\n",
       "      <td>12620.604411</td>\n",
       "      <td>1226.306633</td>\n",
       "      <td>3369.338617</td>\n",
       "      <td>848.213675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PRV51004</td>\n",
       "      <td>0</td>\n",
       "      <td>2.818792</td>\n",
       "      <td>1.604027</td>\n",
       "      <td>1.429530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.516779</td>\n",
       "      <td>0.993289</td>\n",
       "      <td>350.134228</td>\n",
       "      <td>2.080537</td>\n",
       "      <td>...</td>\n",
       "      <td>2430.017927</td>\n",
       "      <td>694.246881</td>\n",
       "      <td>1536.290845</td>\n",
       "      <td>113.086257</td>\n",
       "      <td>0.667719</td>\n",
       "      <td>0.577420</td>\n",
       "      <td>11016.516937</td>\n",
       "      <td>1111.592405</td>\n",
       "      <td>2972.377916</td>\n",
       "      <td>808.138208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PRV51005</td>\n",
       "      <td>1</td>\n",
       "      <td>2.731330</td>\n",
       "      <td>1.599142</td>\n",
       "      <td>1.088412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.783691</td>\n",
       "      <td>0.996567</td>\n",
       "      <td>241.124464</td>\n",
       "      <td>3.175966</td>\n",
       "      <td>...</td>\n",
       "      <td>2195.953526</td>\n",
       "      <td>630.805985</td>\n",
       "      <td>1234.005090</td>\n",
       "      <td>91.141252</td>\n",
       "      <td>0.657071</td>\n",
       "      <td>0.565930</td>\n",
       "      <td>10021.329572</td>\n",
       "      <td>957.701391</td>\n",
       "      <td>2727.944083</td>\n",
       "      <td>737.419878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PRV51007</td>\n",
       "      <td>0</td>\n",
       "      <td>2.736111</td>\n",
       "      <td>1.527778</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>70.583333</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>468.194444</td>\n",
       "      <td>45.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>2089.969651</td>\n",
       "      <td>606.550334</td>\n",
       "      <td>1519.425993</td>\n",
       "      <td>103.302166</td>\n",
       "      <td>0.626542</td>\n",
       "      <td>0.520122</td>\n",
       "      <td>10565.761429</td>\n",
       "      <td>1126.358206</td>\n",
       "      <td>2486.827069</td>\n",
       "      <td>682.279276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Provider  PotentialFraud  phy_same  phy_count    period  periodadmt  \\\n",
       "0  PRV51001               0  2.960000   1.600000  1.440000    1.000000   \n",
       "1  PRV51003               1  2.439394   1.530303  3.674242    2.424242   \n",
       "2  PRV51004               0  2.818792   1.604027  1.429530    0.000000   \n",
       "3  PRV51005               1  2.731330   1.599142  1.088412    0.000000   \n",
       "4  PRV51007               0  2.736111   1.527778  0.958333    0.222222   \n",
       "\n",
       "         age     alife  provider_InscClaimAmtReimbursed_mean  \\\n",
       "0  80.240000  1.000000                           4185.600000   \n",
       "1  71.371212  0.992424                           4588.409091   \n",
       "2  73.516779  0.993289                            350.134228   \n",
       "3  71.783691  0.996567                            241.124464   \n",
       "4  70.583333  0.986111                            468.194444   \n",
       "\n",
       "   provider_DeductibleAmtPaid_mean  ...  diag1_OPAnnualReimbursementAmt_mean  \\\n",
       "0                       213.600000  ...                          1804.331094   \n",
       "1                       502.166667  ...                          2422.458599   \n",
       "2                         2.080537  ...                          2430.017927   \n",
       "3                         3.175966  ...                          2195.953526   \n",
       "4                        45.333333  ...                          2089.969651   \n",
       "\n",
       "   diag1_OPAnnualDeductibleAmt_mean  diag1_InscClaimAmtReimbursed_std  \\\n",
       "0                        543.045084                       3482.066310   \n",
       "1                        676.313985                       4017.871066   \n",
       "2                        694.246881                       1536.290845   \n",
       "3                        630.805985                       1234.005090   \n",
       "4                        606.550334                       1519.425993   \n",
       "\n",
       "   diag1_DeductibleAmtPaid_std  diag1_NoOfMonths_PartACov_std  \\\n",
       "0                   161.353027                       0.569945   \n",
       "1                   260.257069                       0.726572   \n",
       "2                   113.086257                       0.667719   \n",
       "3                    91.141252                       0.657071   \n",
       "4                   103.302166                       0.626542   \n",
       "\n",
       "   diag1_NoOfMonths_PartBCov_std  diag1_IPAnnualReimbursementAmt_std  \\\n",
       "0                       0.424192                        12941.552352   \n",
       "1                       0.653285                        12620.604411   \n",
       "2                       0.577420                        11016.516937   \n",
       "3                       0.565930                        10021.329572   \n",
       "4                       0.520122                        10565.761429   \n",
       "\n",
       "   diag1_IPAnnualDeductibleAmt_std  diag1_OPAnnualReimbursementAmt_std  \\\n",
       "0                      1205.297144                         2450.076771   \n",
       "1                      1226.306633                         3369.338617   \n",
       "2                      1111.592405                         2972.377916   \n",
       "3                       957.701391                         2727.944083   \n",
       "4                      1126.358206                         2486.827069   \n",
       "\n",
       "   diag1_OPAnnualDeductibleAmt_std  \n",
       "0                       661.506672  \n",
       "1                       848.213675  \n",
       "2                       808.138208  \n",
       "3                       737.419878  \n",
       "4                       682.279276  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f56c06e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:06.122633Z",
     "iopub.status.busy": "2023-10-24T17:49:06.121964Z",
     "iopub.status.idle": "2023-10-24T17:49:06.355591Z",
     "shell.execute_reply": "2023-10-24T17:49:06.353987Z"
    },
    "id": "f56c06e1",
    "outputId": "1fec6585-b1ec-4f90-8aea-51a90a38d22f",
    "papermill": {
     "duration": 0.250441,
     "end_time": "2023-10-24T17:49:06.358478",
     "exception": false,
     "start_time": "2023-10-24T17:49:06.108037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Potential Fraud Test distribution\")\n",
    "grouped.groupby( [\"PotentialFraud\"] ).Provider.count().plot(kind = \"bar\", figsize = (10,6))\n",
    "plt.xlabel('Status')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "186272be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:06.392649Z",
     "iopub.status.busy": "2023-10-24T17:49:06.392166Z",
     "iopub.status.idle": "2023-10-24T17:49:06.591835Z",
     "shell.execute_reply": "2023-10-24T17:49:06.590888Z"
    },
    "id": "186272be",
    "outputId": "3cee3255-cd69-418a-9443-e10a8915d099",
    "papermill": {
     "duration": 0.220039,
     "end_time": "2023-10-24T17:49:06.594362",
     "exception": false,
     "start_time": "2023-10-24T17:49:06.374323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAH5CAYAAACPsogXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAtIUlEQVR4nO3de3CV9Z348U8kJJI0nHJpElPQ0opWDTotthhrvRTBG9LqH7aLjbZSL1UUVlx3qdPRznaJ1SnSDqul1kGtWnrTrrO7pmC1WCt4AVMBXWpbFkETEBtCEEgQnt8f/XlmY6iXkJt8X6+ZzPQ855PzfE+esX379DnPKciyLAsAAEjEAX29AAAA6E0CGACApAhgAACSIoABAEiKAAYAICkCGACApAhgAACSUtjXC3i/2LNnT7zyyitRVlYWBQUFfb0cAADeIsuyaG1tjaqqqjjggL9/nlcAv0uvvPJKjBw5sq+XAQDAO1i/fn2MGDHi7z4vgN+lsrKyiPjbH3Tw4MF9vBoAAN5q69atMXLkyHy3/T0C+F1687KHwYMHC2AAgH7snS5X9SE4AACSIoABAEiKAAYAICkCGACApAhgAACSIoABAEiKAAYAICkCGACApAhgAACSIoABAEiKAAYAICkCGACApAhgAACSIoABAEiKAAYAICkCGACApAhgAACSIoABAEhKYV8vgL+vpaUltm/f3mv7KykpiVwu12v7AwDoCwK4n2ppaYl/vflfY/O2zb22z+EfGB7f/KdvimAAYL8mgPup7du3x+Ztm2PQmEFR8sGSnt/flu2xeeXm2L59uwAGAPZrArifK/lgSZQNK+uVfe2IHb2yHwCAvuRDcAAAJEUAAwCQFAEMAEBSBDAAAEkRwAAAJEUAAwCQFAEMAEBSBDAAAEkRwAAAJEUAAwCQFAEMAEBSBDAAAEkRwAAAJEUAAwCQFAEMAEBSBDAAAEkRwAAAJEUAAwCQFAEMAEBSBDAAAEkRwAAAJEUAAwCQFAEMAEBSBDAAAEkRwAAAJEUAAwCQFAEMAEBSBDAAAEkRwAAAJEUAAwCQFAEMAEBSBDAAAEkRwAAAJEUAAwCQFAEMAEBSBDAAAEkRwAAAJEUAAwCQFAEMAEBSBDAAAEkRwAAAJEUAAwCQlH4TwHV1dVFQUBAzZszIb8uyLG644YaoqqqKQYMGxcknnxyrV6/u8HttbW1x5ZVXxvDhw6O0tDQmT54cGzZs6DDT3NwctbW1kcvlIpfLRW1tbWzZsqUX3hUAAP1Nvwjgp59+On74wx/G0Ucf3WH7TTfdFHPmzIl58+bF008/HZWVlTFhwoRobW3Nz8yYMSMeeOCBWLhwYTz++OOxbdu2mDRpUuzevTs/M2XKlGhoaIj6+vqor6+PhoaGqK2t7bX3BwBA/9HnAbxt27Y4//zz4/bbb48hQ4bkt2dZFnPnzo3rrrsuzj333Kiuro677rortm/fHvfdd19ERLS0tMQdd9wR3/3ud+PUU0+NT3ziE3HPPffEypUr4+GHH46IiBdeeCHq6+vjRz/6UdTU1ERNTU3cfvvt8Z//+Z+xZs2aPnnPAAD0nT4P4CuuuCLOOuusOPXUUztsX7t2bTQ1NcXEiRPz24qLi+Okk06KJ554IiIili9fHrt27eowU1VVFdXV1fmZpUuXRi6Xi3HjxuVnjjvuuMjlcvmZvWlra4utW7d2+AEA4P2vsC93vnDhwlixYkU8/fTTnZ5ramqKiIiKiooO2ysqKmLdunX5maKiog5njt+cefP3m5qaory8vNPrl5eX52f2pq6uLr71rW+9tzcEAEC/12dngNevXx/Tp0+Pe+65Jw488MC/O1dQUNDhcZZlnba91Vtn9jb/Tq8za9asaGlpyf+sX7/+bfcJAMD7Q58F8PLly2PTpk0xduzYKCwsjMLCwliyZEl8//vfj8LCwvyZ37eepd20aVP+ucrKymhvb4/m5ua3ndm4cWOn/b/66qudzi7/X8XFxTF48OAOPwAAvP/1WQCPHz8+Vq5cGQ0NDfmfY489Ns4///xoaGiIj370o1FZWRmLFy/O/057e3ssWbIkjj/++IiIGDt2bAwcOLDDTGNjY6xatSo/U1NTEy0tLfHUU0/lZ5588sloaWnJzwAAkI4+uwa4rKwsqqurO2wrLS2NYcOG5bfPmDEjZs+eHaNHj47Ro0fH7Nmzo6SkJKZMmRIREblcLqZOnRozZ86MYcOGxdChQ+Oaa66JMWPG5D9Ud8QRR8Tpp58eF198ccyfPz8iIi655JKYNGlSHH744b34jgEA6A/69ENw7+Taa6+NHTt2xOWXXx7Nzc0xbty4WLRoUZSVleVnbrnlligsLIzzzjsvduzYEePHj48777wzBgwYkJ+5995746qrrsrfLWLy5Mkxb968Xn8/AAD0vYIsy7K+XsT7wdatWyOXy0VLS0uvXA/c2NgYs74zK4Z9dliUDSt751/YR62vtcZrv3st6v65Lg466KAe3x8AQHd7t73W5/cBBgCA3iSAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApAhgAgKQIYAAAkiKAAQBIigAGACApfRrAt912Wxx99NExePDgGDx4cNTU1MRDDz2Ufz7LsrjhhhuiqqoqBg0aFCeffHKsXr26w2u0tbXFlVdeGcOHD4/S0tKYPHlybNiwocNMc3Nz1NbWRi6Xi1wuF7W1tbFly5beeIsAAPQzfRrAI0aMiBtvvDGeeeaZeOaZZ+Jzn/tcfP7zn89H7k033RRz5syJefPmxdNPPx2VlZUxYcKEaG1tzb/GjBkz4oEHHoiFCxfG448/Htu2bYtJkybF7t278zNTpkyJhoaGqK+vj/r6+mhoaIja2tpef78AAPS9wr7c+dlnn93h8b/927/FbbfdFsuWLYsjjzwy5s6dG9ddd12ce+65ERFx1113RUVFRdx3331x6aWXRktLS9xxxx3x4x//OE499dSIiLjnnnti5MiR8fDDD8dpp50WL7zwQtTX18eyZcti3LhxERFx++23R01NTaxZsyYOP/zw3n3TAAD0qX5zDfDu3btj4cKF8frrr0dNTU2sXbs2mpqaYuLEifmZ4uLiOOmkk+KJJ56IiIjly5fHrl27OsxUVVVFdXV1fmbp0qWRy+Xy8RsRcdxxx0Uul8vP7E1bW1ts3bq1ww8AAO9/fR7AK1eujA984ANRXFwcl112WTzwwANx5JFHRlNTU0REVFRUdJivqKjIP9fU1BRFRUUxZMiQt50pLy/vtN/y8vL8zN7U1dXlrxnO5XIxcuTIfXqfAAD0D30ewIcffng0NDTEsmXL4utf/3pceOGF8fzzz+efLygo6DCfZVmnbW/11pm9zb/T68yaNStaWlryP+vXr3+3bwkAgH6szwO4qKgoDj300Dj22GOjrq4ujjnmmPje974XlZWVERGdztJu2rQpf1a4srIy2tvbo7m5+W1nNm7c2Gm/r776aqezy/9XcXFx/u4Ub/4AAPD+1+cB/FZZlkVbW1uMGjUqKisrY/Hixfnn2tvbY8mSJXH88cdHRMTYsWNj4MCBHWYaGxtj1apV+ZmamppoaWmJp556Kj/z5JNPRktLS34GAIB09OldIL7xjW/EGWecESNHjozW1tZYuHBh/Pa3v436+vooKCiIGTNmxOzZs2P06NExevTomD17dpSUlMSUKVMiIiKXy8XUqVNj5syZMWzYsBg6dGhcc801MWbMmPxdIY444og4/fTT4+KLL4758+dHRMQll1wSkyZNcgcIAIAE9WkAb9y4MWpra6OxsTFyuVwcffTRUV9fHxMmTIiIiGuvvTZ27NgRl19+eTQ3N8e4ceNi0aJFUVZWln+NW265JQoLC+O8886LHTt2xPjx4+POO++MAQMG5GfuvffeuOqqq/J3i5g8eXLMmzevd98sAAD9QkGWZVlfL+L9YOvWrZHL5aKlpaVXrgdubGyMWd+ZFcM+OyzKhpW98y/so9bXWuO1370Wdf9cFwcddFCP7w8AoLu9217rd9cAAwBATxLAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJCULgXw2rVru3sdAADQK7oUwIceemiccsopcc8998TOnTu7e00AANBjuhTAf/jDH+ITn/hEzJw5MyorK+PSSy+Np556qrvXBgAA3a5LAVxdXR1z5syJl19+ORYsWBBNTU1xwgknxFFHHRVz5syJV199tbvXCQAA3WKfPgRXWFgY55xzTvzsZz+L73znO/HnP/85rrnmmhgxYkRccMEF0djY2F3rBACAbrFPAfzMM8/E5ZdfHgcddFDMmTMnrrnmmvjzn/8cjzzySLz88svx+c9/vrvWCQAA3aKwK780Z86cWLBgQaxZsybOPPPMuPvuu+PMM8+MAw74W0+PGjUq5s+fHx//+Me7dbEAALCvuhTAt912W1x00UXx1a9+NSorK/c6c/DBB8cdd9yxT4sDAIDu1qUAfvHFF99xpqioKC688MKuvDwAAPSYLl0DvGDBgvj5z3/eafvPf/7zuOuuu/Z5UQAA0FO6FMA33nhjDB8+vNP28vLymD179j4vCgAAekqXAnjdunUxatSoTtsPOeSQeOmll/Z5UQAA0FO6FMDl5eXx3HPPddr+hz/8IYYNG7bPiwIAgJ7SpQD+0pe+FFdddVU8+uijsXv37ti9e3c88sgjMX369PjSl77U3WsEAIBu06W7QHz729+OdevWxfjx46Ow8G8vsWfPnrjgggtcAwwAQL/WpQAuKiqKn/70p/Gv//qv8Yc//CEGDRoUY8aMiUMOOaS71wcAAN2qSwH8psMOOywOO+yw7loLAAD0uC4F8O7du+POO++M3/zmN7Fp06bYs2dPh+cfeeSRblkcAAB0ty4F8PTp0+POO++Ms846K6qrq6OgoKC71wUAAD2iSwG8cOHC+NnPfhZnnnlmd68HAAB6VJdug1ZUVBSHHnpod68FAAB6XJcCeObMmfG9730vsizr7vUAAECP6tIlEI8//ng8+uij8dBDD8VRRx0VAwcO7PD8/fff3y2LAwCA7talAP7gBz8Y55xzTnevBQAAelyXAnjBggXdvQ4AAOgVXboGOCLijTfeiIcffjjmz58fra2tERHxyiuvxLZt27ptcQAA0N26dAZ43bp1cfrpp8dLL70UbW1tMWHChCgrK4ubbropdu7cGT/4wQ+6e50AANAtunQGePr06XHsscdGc3NzDBo0KL/9nHPOid/85jfdtjgAAOhuXb4LxO9///soKirqsP2QQw6Jl19+uVsWBgAAPaFLZ4D37NkTu3fv7rR9w4YNUVZWts+LAgCAntKlAJ4wYULMnTs3/7igoCC2bdsW119/va9HBgCgX+vSJRC33HJLnHLKKXHkkUfGzp07Y8qUKfHiiy/G8OHD4yc/+Ul3rxEAALpNlwK4qqoqGhoa4ic/+UmsWLEi9uzZE1OnTo3zzz+/w4fiAACgv+lSAEdEDBo0KC666KK46KKLunM9AADQo7oUwHfffffbPn/BBRd0aTEAANDTuhTA06dP7/B4165dsX379igqKoqSkhIBDABAv9Wlu0A0Nzd3+Nm2bVusWbMmTjjhBB+CAwCgX+tSAO/N6NGj48Ybb+x0dhgAAPqTbgvgiIgBAwbEK6+80p0vCQAA3apL1wA/+OCDHR5nWRaNjY0xb968+MxnPtMtCwMAgJ7QpQD+whe+0OFxQUFBfOhDH4rPfe5z8d3vfrc71gUAAD2iSwG8Z8+e7l4HAAD0im69BhgAAPq7Lp0Bvvrqq9/17Jw5c7qyCwAA6BFdCuBnn302VqxYEW+88UYcfvjhERHxxz/+MQYMGBCf/OQn83MFBQXds0oAAOgmXQrgs88+O8rKyuKuu+6KIUOGRMTfvhzjq1/9anz2s5+NmTNndusiAQCgu3TpGuDvfve7UVdXl4/fiIghQ4bEt7/9bXeBAACgX+tSAG/dujU2btzYafumTZuitbV1nxcFAAA9pUsBfM4558RXv/rV+MUvfhEbNmyIDRs2xC9+8YuYOnVqnHvuud29RgAA6DZdugb4Bz/4QVxzzTXx5S9/OXbt2vW3FyosjKlTp8bNN9/crQsEAIDu1KUALikpiVtvvTVuvvnm+POf/xxZlsWhhx4apaWl3b0+AADoVvv0RRiNjY3R2NgYhx12WJSWlkaWZd21LgAA6BFdCuDXXnstxo8fH4cddliceeaZ0djYGBERX/va19wCDQCAfq1LAfyP//iPMXDgwHjppZeipKQkv/2LX/xi1NfXd9viAACgu3XpGuBFixbFr3/96xgxYkSH7aNHj45169Z1y8IAAKAndOkM8Ouvv97hzO+bNm/eHMXFxfu8KAAA6CldCuATTzwx7r777vzjgoKC2LNnT9x8881xyimndNviAACgu3XpEoibb745Tj755HjmmWeivb09rr322li9enX89a9/jd///vfdvUYAAOg2XToDfOSRR8Zzzz0Xn/70p2PChAnx+uuvx7nnnhvPPvtsfOxjH+vuNQIAQLd5z2eAd+3aFRMnToz58+fHt771rZ5YEwAA9Jj3fAZ44MCBsWrVqigoKOiJ9QAAQI/q0iUQF1xwQdxxxx3dvRYAAOhxXfoQXHt7e/zoRz+KxYsXx7HHHhulpaUdnp8zZ063LA4AALrbewrgv/zlL/GRj3wkVq1aFZ/85CcjIuKPf/xjhxmXRgAA0J+9p0sgRo8eHZs3b45HH300Hn300SgvL4+FCxfmHz/66KPxyCOPvOvXq6uri0996lNRVlYW5eXl8YUvfCHWrFnTYSbLsrjhhhuiqqoqBg0aFCeffHKsXr26w0xbW1tceeWVMXz48CgtLY3JkyfHhg0bOsw0NzdHbW1t5HK5yOVyUVtbG1u2bHkvbx8AgP3AewrgLMs6PH7ooYfi9ddf7/LOlyxZEldccUUsW7YsFi9eHG+88UZMnDixw2vedNNNMWfOnJg3b148/fTTUVlZGRMmTIjW1tb8zIwZM+KBBx6IhQsXxuOPPx7btm2LSZMmxe7du/MzU6ZMiYaGhqivr4/6+vpoaGiI2traLq8dAID3py5dA/ymtwbxe1VfX9/h8YIFC6K8vDyWL18eJ554YmRZFnPnzo3rrrsuzj333IiIuOuuu6KioiLuu+++uPTSS6OlpSXuuOOO+PGPfxynnnpqRETcc889MXLkyHj44YfjtNNOixdeeCHq6+tj2bJlMW7cuIiIuP3226OmpibWrFkThx9+eKe1tbW1RVtbW/7x1q1b9+m9AgDQP7ynM8AFBQWdrvHtzmt+W1paIiJi6NChERGxdu3aaGpqiokTJ+ZniouL46STToonnngiIiKWL1+evzfxm6qqqqK6ujo/s3Tp0sjlcvn4jYg47rjjIpfL5Wfeqq6uLn+5RC6Xi5EjR3bb+wQAoO+8pzPAWZbFV77ylSguLo6IiJ07d8Zll13W6S4Q999//3teSJZlcfXVV8cJJ5wQ1dXVERHR1NQUEREVFRUdZisqKmLdunX5maKiohgyZEinmTd/v6mpKcrLyzvts7y8PD/zVrNmzYqrr746/3jr1q0iGABgP/CeAvjCCy/s8PjLX/5yty1k2rRp8dxzz8Xjjz/e6bm3nmXOsuwdzzy/dWZv82/3OsXFxfnQBwBg//GeAnjBggU9sogrr7wyHnzwwXjsscdixIgR+e2VlZUR8bczuAcddFB++6ZNm/JnhSsrK6O9vT2am5s7nAXetGlTHH/88fmZjRs3dtrvq6++2unsMgAA+7cufRNcd8myLKZNmxb3339/PPLIIzFq1KgOz48aNSoqKytj8eLF+W3t7e2xZMmSfNyOHTs2Bg4c2GGmsbExVq1alZ+pqamJlpaWeOqpp/IzTz75ZLS0tORnAABIwz7dBWJfXXHFFXHffffFf/zHf0RZWVn+etxcLheDBg2KgoKCmDFjRsyePTtGjx4do0ePjtmzZ0dJSUlMmTIlPzt16tSYOXNmDBs2LIYOHRrXXHNNjBkzJn9XiCOOOCJOP/30uPjii2P+/PkREXHJJZfEpEmT9noHCAAA9l99GsC33XZbREScfPLJHbYvWLAgvvKVr0RExLXXXhs7duyIyy+/PJqbm2PcuHGxaNGiKCsry8/fcsstUVhYGOedd17s2LEjxo8fH3feeWcMGDAgP3PvvffGVVddlb9bxOTJk2PevHk9+wYBAOh3CrJ9vZlvIrZu3Rq5XC5aWlpi8ODBPb6/xsbGmPWdWTHss8OibFjZO//CPmp9rTVe+91rUffPdR2utwYAeL94t73Wp9cAAwBAbxPAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJCUPg3gxx57LM4+++yoqqqKgoKC+NWvftXh+SzL4oYbboiqqqoYNGhQnHzyybF69eoOM21tbXHllVfG8OHDo7S0NCZPnhwbNmzoMNPc3By1tbWRy+Uil8tFbW1tbNmypYffHQAA/VGfBvDrr78exxxzTMybN2+vz990000xZ86cmDdvXjz99NNRWVkZEyZMiNbW1vzMjBkz4oEHHoiFCxfG448/Htu2bYtJkybF7t278zNTpkyJhoaGqK+vj/r6+mhoaIja2toef38AAPQ/hX258zPOOCPOOOOMvT6XZVnMnTs3rrvuujj33HMjIuKuu+6KioqKuO++++LSSy+NlpaWuOOOO+LHP/5xnHrqqRERcc8998TIkSPj4YcfjtNOOy1eeOGFqK+vj2XLlsW4ceMiIuL222+PmpqaWLNmTRx++OF73X9bW1u0tbXlH2/durU73zoAAH2k314DvHbt2mhqaoqJEyfmtxUXF8dJJ50UTzzxRERELF++PHbt2tVhpqqqKqqrq/MzS5cujVwul4/fiIjjjjsucrlcfmZv6urq8pdM5HK5GDlyZHe/RQAA+kC/DeCmpqaIiKioqOiwvaKiIv9cU1NTFBUVxZAhQ952pry8vNPrl5eX52f2ZtasWdHS0pL/Wb9+/T69HwAA+oc+vQTi3SgoKOjwOMuyTtve6q0ze5t/p9cpLi6O4uLi97haAAD6u357BriysjIiotNZ2k2bNuXPCldWVkZ7e3s0Nze/7czGjRs7vf6rr77a6ewyAAD7v34bwKNGjYrKyspYvHhxflt7e3ssWbIkjj/++IiIGDt2bAwcOLDDTGNjY6xatSo/U1NTEy0tLfHUU0/lZ5588sloaWnJzwAAkI4+vQRi27Zt8ac//Sn/eO3atdHQ0BBDhw6Ngw8+OGbMmBGzZ8+O0aNHx+jRo2P27NlRUlISU6ZMiYiIXC4XU6dOjZkzZ8awYcNi6NChcc0118SYMWPyd4U44ogj4vTTT4+LL7445s+fHxERl1xySUyaNOnv3gECAID9V58G8DPPPBOnnHJK/vHVV18dEREXXnhh3HnnnXHttdfGjh074vLLL4/m5uYYN25cLFq0KMrKyvK/c8stt0RhYWGcd955sWPHjhg/fnzceeedMWDAgPzMvffeG1dddVX+bhGTJ0/+u/ceBgBg/1aQZVnW14t4P9i6dWvkcrloaWmJwYMH9/j+GhsbY9Z3ZsWwzw6LsmFl7/wL+6j1tdZ47XevRd0/18VBBx3U4/sDAOhu77bX+u01wAAA0BMEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkpbCvF0D/0d7WHhs3buy1/ZWUlEQul+u1/QEARAhg/r+219viueeei9m3zo6SkpJe2efwDwyPb/7TN0UwANCrBDAREbGrfVfszHbGgdUHxrAPD+vx/W3fsj02r9wc27dvF8AAQK8SwHQwKDcoyoaV9cq+dsSOXtkPAMD/5UNwAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUAQwAQFIEMAAASRHAAAAkRQADAJAUX4VMn2lva4+NGzf26j5LSkoil8v16j4BgP5FANMn2l5vi+eeey5m3zo7SkpKem2/wz8wPL75T98UwQCQMAFMn9jVvit2ZjvjwOoDY9iHh/XKPrdv2R6bV26O7du3C2AASJgApk8Nyg2KsmFlvba/HbGj1/YFAPRPPgQHAEBSBDAAAEkRwAAAJEUAAwCQFAEMAEBSBDAAAEkRwAAAJMV9gElKb3/9sq9eBoD+RwCTjL74+mVfvQwA/Y8AJhm9/fXLvnoZAPonAUxyevPrl331MgD0Pz4EBwBAUgQwAABJcQkE9CB3nQCA/kcAQw9x1wkA6J8EMPQQd50AgP5JAEMPc9cJAOhffAgOAICkJHUG+NZbb42bb745Ghsb46ijjoq5c+fGZz/72b5eFnQbH7oDgHeWTAD/9Kc/jRkzZsStt94an/nMZ2L+/PlxxhlnxPPPPx8HH3xwXy8P9llffOiurLAspl08LQYPHtwr+9u1a1cMHDiwV/YVIfAB9lfJBPCcOXNi6tSp8bWvfS0iIubOnRu//vWv47bbbou6urpO821tbdHW1pZ/3NLSEhERW7du7ZX1tra2Rntbe2xp3BJt29ve+Rf2UcvGltjzxp5o2dgSAw/o+cDo7f31xT57e3+b12+O7bu2R3t5e5R+qLTH99e6uTWeXPRkbNi8IQYNGtTj+9vVviv+tOZPcejHD+21CB48cHBcfOHFUVbWO9dwR0RkWRYFBQX2Z3/9dp/2Z3/vVWlpaa/99+ibnZZl2dvOFWTvNLEfaG9vj5KSkvj5z38e55xzTn779OnTo6GhIZYsWdLpd2644Yb41re+1ZvLBACgG6xfvz5GjBjxd59P4gzw5s2bY/fu3VFRUdFhe0VFRTQ1Ne31d2bNmhVXX311/vGePXvir3/9awwbNqxX/s1p69atMXLkyFi/fn2v/d/L7J1j0X84Fv2HY9E/OA79h2PRP2RZFq2trVFVVfW2c0kE8JveGq5v938DFBcXR3FxcYdtH/zgB3tqaX/X4MGD/YPUTzgW/Ydj0X84Fv2D49B/OBZ97918diOJ26ANHz48BgwY0Ols76ZNmzqdFQYAYP+WRAAXFRXF2LFjY/HixR22L168OI4//vg+WhUAAH0hmUsgrr766qitrY1jjz02ampq4oc//GG89NJLcdlll/X10vaquLg4rr/++k6XYdD7HIv+w7HoPxyL/sFx6D8ci/eXJO4C8aZbb701brrppmhsbIzq6uq45ZZb4sQTT+zrZQEA0IuSCmAAAEjiGmAAAHiTAAYAICkCGACApAhgAACSIoD7oVtvvTVGjRoVBx54YIwdOzZ+97vf9fWS3lcee+yxOPvss6OqqioKCgriV7/6VYfnsyyLG264IaqqqmLQoEFx8sknx+rVqzvMtLW1xZVXXhnDhw+P0tLSmDx5cmzYsKHDTHNzc9TW1kYul4tcLhe1tbWxZcuWDjMvvfRSnH322VFaWhrDhw+Pq666Ktrb23vibfdLdXV18alPfSrKysqivLw8vvCFL8SaNWs6zDgePe+2226Lo48+Ov8NVTU1NfHQQw/ln3cM+k5dXV0UFBTEjBkz8tscj95xww03REFBQYefysrK/POOw34uo19ZuHBhNnDgwOz222/Pnn/++Wz69OlZaWlptm7dur5e2vvGf//3f2fXXXdd9stf/jKLiOyBBx7o8PyNN96YlZWVZb/85S+zlStXZl/84hezgw46KNu6dWt+5rLLLss+/OEPZ4sXL85WrFiRnXLKKdkxxxyTvfHGG/mZ008/Pauurs6eeOKJ7Iknnsiqq6uzSZMm5Z9/4403surq6uyUU07JVqxYkS1evDirqqrKpk2b1uN/g/7itNNOyxYsWJCtWrUqa2hoyM4666zs4IMPzrZt25afcTx63oMPPpj913/9V7ZmzZpszZo12Te+8Y1s4MCB2apVq7Iscwz6ylNPPZV95CMfyY4++uhs+vTp+e2OR++4/vrrs6OOOiprbGzM/2zatCn/vOOwfxPA/cynP/3p7LLLLuuw7eMf/3j2L//yL320ove3twbwnj17ssrKyuzGG2/Mb9u5c2eWy+WyH/zgB1mWZdmWLVuygQMHZgsXLszPvPzyy9kBBxyQ1dfXZ1mWZc8//3wWEdmyZcvyM0uXLs0iIvuf//mfLMv+FuIHHHBA9vLLL+dnfvKTn2TFxcVZS0tLj7zf/m7Tpk1ZRGRLlizJsszx6EtDhgzJfvSjHzkGfaS1tTUbPXp0tnjx4uykk07KB7Dj0Xuuv/767Jhjjtnrc47D/s8lEP1Ie3t7LF++PCZOnNhh+8SJE+OJJ57oo1XtX9auXRtNTU0d/sbFxcVx0kkn5f/Gy5cvj127dnWYqaqqiurq6vzM0qVLI5fLxbhx4/Izxx13XORyuQ4z1dXVUVVVlZ857bTToq2tLZYvX96j77O/amlpiYiIoUOHRoTj0Rd2794dCxcujNdffz1qamocgz5yxRVXxFlnnRWnnnpqh+2OR+968cUXo6qqKkaNGhVf+tKX4i9/+UtEOA4pSOarkN8PNm/eHLt3746KiooO2ysqKqKpqamPVrV/efPvuLe/8bp16/IzRUVFMWTIkE4zb/5+U1NTlJeXd3r98vLyDjNv3c+QIUOiqKgoyeOZZVlcffXVccIJJ0R1dXVEOB69aeXKlVFTUxM7d+6MD3zgA/HAAw/EkUcemf8fYceg9yxcuDBWrFgRTz/9dKfn/DPRe8aNGxd33313HHbYYbFx48b49re/Hccff3ysXr3acUiAAO6HCgoKOjzOsqzTNvZNV/7Gb53Z23xXZlIxbdq0eO655+Lxxx/v9Jzj0fMOP/zwaGhoiC1btsQvf/nLuPDCC2PJkiX55x2D3rF+/fqYPn16LFq0KA488MC/O+d49Lwzzjgj/5/HjBkTNTU18bGPfSzuuuuuOO644yLCcdifuQSiHxk+fHgMGDCg07/xbdq0qdO/HdI1b37C9+3+xpWVldHe3h7Nzc1vO7Nx48ZOr//qq692mHnrfpqbm2PXrl3JHc8rr7wyHnzwwXj00UdjxIgR+e2OR+8pKiqKQw89NI499tioq6uLY445Jr73ve85Br1s+fLlsWnTphg7dmwUFhZGYWFhLFmyJL7//e9HYWFh/u/gePS+0tLSGDNmTLz44ov+uUiAAO5HioqKYuzYsbF48eIO2xcvXhzHH398H61q/zJq1KiorKzs8Ddub2+PJUuW5P/GY8eOjYEDB3aYaWxsjFWrVuVnampqoqWlJZ566qn8zJNPPhktLS0dZlatWhWNjY35mUWLFkVxcXGMHTu2R99nf5FlWUybNi3uv//+eOSRR2LUqFEdnnc8+k6WZdHW1uYY9LLx48fHypUro6GhIf9z7LHHxvnnnx8NDQ3x0Y9+1PHoI21tbfHCCy/EQQcd5J+LFPTe5+14N968Ddodd9yRPf/889mMGTOy0tLS7H//93/7emnvG62trdmzzz6bPfvss1lEZHPmzMmeffbZ/K3kbrzxxiyXy2X3339/tnLlyuwf/uEf9nprmxEjRmQPP/xwtmLFiuxzn/vcXm9tc/TRR2dLly7Nli5dmo0ZM2avt7YZP358tmLFiuzhhx/ORowYkdStbb7+9a9nuVwu++1vf9vhVkPbt2/PzzgePW/WrFnZY489lq1duzZ77rnnsm984xvZAQcckC1atCjLMsegr/3fu0BkmePRW2bOnJn99re/zf7yl79ky5YtyyZNmpSVlZXl//fWcdi/CeB+6N///d+zQw45JCsqKso++clP5m8Zxbvz6KOPZhHR6efCCy/Msuxvt7e5/vrrs8rKyqy4uDg78cQTs5UrV3Z4jR07dmTTpk3Lhg4dmg0aNCibNGlS9tJLL3WYee2117Lzzz8/Kysry8rKyrLzzz8/a25u7jCzbt267KyzzsoGDRqUDR06NJs2bVq2c+fOnnz7/crejkNEZAsWLMjPOB4976KLLsr/d8qHPvShbPz48fn4zTLHoK+9NYAdj97x5n19Bw4cmFVVVWXnnntutnr16vzzjsP+rSDLsqxvzj0DAEDvcw0wAABJEcAAACRFAAMAkBQBDABAUgQwAABJEcAAACRFAAMAkBQBDABAUgQwAABJEcAAACRFAAMAkJT/B6Km33ZpVSviAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = grouped['provider_InscClaimAmtReimbursed_mean'].plot.hist(bins=20, alpha=0.5, figsize=(8, 6), facecolor='g', edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b26ca3cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:06.623053Z",
     "iopub.status.busy": "2023-10-24T17:49:06.622645Z",
     "iopub.status.idle": "2023-10-24T17:49:06.629978Z",
     "shell.execute_reply": "2023-10-24T17:49:06.629238Z"
    },
    "id": "b26ca3cd",
    "papermill": {
     "duration": 0.02458,
     "end_time": "2023-10-24T17:49:06.632461",
     "exception": false,
     "start_time": "2023-10-24T17:49:06.607881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped=grouped.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bee273c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.to_csv(\"Pre Processed Dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "73bea12c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:06.660838Z",
     "iopub.status.busy": "2023-10-24T17:49:06.659531Z",
     "iopub.status.idle": "2023-10-24T17:49:06.666379Z",
     "shell.execute_reply": "2023-10-24T17:49:06.664770Z"
    },
    "id": "73bea12c",
    "papermill": {
     "duration": 0.023293,
     "end_time": "2023-10-24T17:49:06.668863",
     "exception": false,
     "start_time": "2023-10-24T17:49:06.645570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features=grouped.iloc[:,2:]\n",
    "labels=grouped.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "23bef67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      phy_same  phy_count  period  periodadmt   age  alife  \\\n",
      "0         2.96       1.60    1.44        1.00 80.24   1.00   \n",
      "1         2.44       1.53    3.67        2.42 71.37   0.99   \n",
      "2         2.82       1.60    1.43        0.00 73.52   0.99   \n",
      "3         2.73       1.60    1.09        0.00 71.78   1.00   \n",
      "4         2.74       1.53    0.96        0.22 70.58   0.99   \n",
      "...        ...        ...     ...         ...   ...    ...   \n",
      "9803      2.28       1.82    5.88        5.88 80.96   0.92   \n",
      "9804      2.87       1.71    3.90        3.65 75.21   0.99   \n",
      "9805      2.65       1.54    1.92        0.74 75.68   1.00   \n",
      "9806      2.90       1.55    1.55        0.00 75.41   0.99   \n",
      "9807      2.29       1.76    5.08        5.08 74.35   1.00   \n",
      "\n",
      "      provider_InscClaimAmtReimbursed_mean  provider_DeductibleAmtPaid_mean  \\\n",
      "0                                  4185.60                           213.60   \n",
      "1                                  4588.41                           502.17   \n",
      "2                                   350.13                             2.08   \n",
      "3                                   241.12                             3.18   \n",
      "4                                   468.19                            45.33   \n",
      "...                                    ...                              ...   \n",
      "9803                              10580.07                          1068.00   \n",
      "9804                               7374.80                           742.19   \n",
      "9805                               1390.60                           145.96   \n",
      "9806                                278.18                             2.44   \n",
      "9807                               8187.34                          1068.00   \n",
      "\n",
      "      provider_NoOfMonths_PartACov_mean  provider_NoOfMonths_PartBCov_mean  \\\n",
      "0                                 12.00                              12.00   \n",
      "1                                 11.82                              11.87   \n",
      "2                                 11.87                              11.96   \n",
      "3                                 11.91                              11.94   \n",
      "4                                 11.83                              11.83   \n",
      "...                                 ...                                ...   \n",
      "9803                              12.00                              12.00   \n",
      "9804                              11.99                              11.95   \n",
      "9805                              11.96                              11.92   \n",
      "9806                              11.98                              11.93   \n",
      "9807                              11.91                              11.84   \n",
      "\n",
      "      ...  diag1_OPAnnualReimbursementAmt_mean  \\\n",
      "0     ...                              1804.33   \n",
      "1     ...                              2422.46   \n",
      "2     ...                              2430.02   \n",
      "3     ...                              2195.95   \n",
      "4     ...                              2089.97   \n",
      "...   ...                                  ...   \n",
      "9803  ...                              2072.24   \n",
      "9804  ...                              2128.93   \n",
      "9805  ...                              2200.00   \n",
      "9806  ...                              2364.82   \n",
      "9807  ...                              2117.86   \n",
      "\n",
      "      diag1_OPAnnualDeductibleAmt_mean  diag1_InscClaimAmtReimbursed_std  \\\n",
      "0                               543.05                           3482.07   \n",
      "1                               676.31                           4017.87   \n",
      "2                               694.25                           1536.29   \n",
      "3                               630.81                           1234.01   \n",
      "4                               606.55                           1519.43   \n",
      "...                                ...                               ...   \n",
      "9803                            594.63                           6284.03   \n",
      "9804                            596.20                           5433.97   \n",
      "9805                            624.14                           1945.10   \n",
      "9806                            667.59                           1329.18   \n",
      "9807                            589.51                           6696.34   \n",
      "\n",
      "      diag1_DeductibleAmtPaid_std  diag1_NoOfMonths_PartACov_std  \\\n",
      "0                          161.35                           0.57   \n",
      "1                          260.26                           0.73   \n",
      "2                          113.09                           0.67   \n",
      "3                           91.14                           0.66   \n",
      "4                          103.30                           0.63   \n",
      "...                           ...                            ...   \n",
      "9803                       423.17                           0.92   \n",
      "9804                       319.80                           0.77   \n",
      "9805                       132.03                           0.69   \n",
      "9806                        98.90                           0.64   \n",
      "9807                       436.35                           0.87   \n",
      "\n",
      "      diag1_NoOfMonths_PartBCov_std  diag1_IPAnnualReimbursementAmt_std  \\\n",
      "0                              0.42                            12941.55   \n",
      "1                              0.65                            12620.60   \n",
      "2                              0.58                            11016.52   \n",
      "3                              0.57                            10021.33   \n",
      "4                              0.52                            10565.76   \n",
      "...                             ...                                 ...   \n",
      "9803                           0.62                            14054.42   \n",
      "9804                           0.63                            13896.80   \n",
      "9805                           0.59                            10983.07   \n",
      "9806                           0.56                            10405.45   \n",
      "9807                           0.71                            14089.52   \n",
      "\n",
      "      diag1_IPAnnualDeductibleAmt_std  diag1_OPAnnualReimbursementAmt_std  \\\n",
      "0                             1205.30                             2450.08   \n",
      "1                             1226.31                             3369.34   \n",
      "2                             1111.59                             2972.38   \n",
      "3                              957.70                             2727.94   \n",
      "4                             1126.36                             2486.83   \n",
      "...                               ...                                 ...   \n",
      "9803                          1537.49                             3344.99   \n",
      "9804                          1319.65                             3287.32   \n",
      "9805                          1081.73                             2858.76   \n",
      "9806                          1006.94                             2858.80   \n",
      "9807                          1331.46                             3418.98   \n",
      "\n",
      "      diag1_OPAnnualDeductibleAmt_std  \n",
      "0                              661.51  \n",
      "1                              848.21  \n",
      "2                              808.14  \n",
      "3                              737.42  \n",
      "4                              682.28  \n",
      "...                               ...  \n",
      "9803                           842.14  \n",
      "9804                           817.25  \n",
      "9805                           760.01  \n",
      "9806                           766.22  \n",
      "9807                           895.87  \n",
      "\n",
      "[9808 rows x 54 columns]\n"
     ]
    }
   ],
   "source": [
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7a5ae0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       1\n",
      "2       0\n",
      "3       1\n",
      "4       0\n",
      "       ..\n",
      "9803    1\n",
      "9804    1\n",
      "9805    1\n",
      "9806    1\n",
      "9807    1\n",
      "Name: PotentialFraud, Length: 9808, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed5184bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:06.696166Z",
     "iopub.status.busy": "2023-10-24T17:49:06.695695Z",
     "iopub.status.idle": "2023-10-24T17:49:07.823294Z",
     "shell.execute_reply": "2023-10-24T17:49:07.821532Z"
    },
    "id": "ed5184bb",
    "papermill": {
     "duration": 1.14437,
     "end_time": "2023-10-24T17:49:07.826137",
     "exception": false,
     "start_time": "2023-10-24T17:49:06.681767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "features, labels = oversample.fit_resample(features, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5b3fdb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:07.855991Z",
     "iopub.status.busy": "2023-10-24T17:49:07.854713Z",
     "iopub.status.idle": "2023-10-24T17:49:07.872316Z",
     "shell.execute_reply": "2023-10-24T17:49:07.871028Z"
    },
    "id": "e5b3fdb3",
    "papermill": {
     "duration": 0.035108,
     "end_time": "2023-10-24T17:49:07.874657",
     "exception": false,
     "start_time": "2023-10-24T17:49:07.839549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "featuresstand=scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f628061",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:07.902778Z",
     "iopub.status.busy": "2023-10-24T17:49:07.901799Z",
     "iopub.status.idle": "2023-10-24T17:49:07.914531Z",
     "shell.execute_reply": "2023-10-24T17:49:07.913672Z"
    },
    "id": "6f628061",
    "papermill": {
     "duration": 0.029492,
     "end_time": "2023-10-24T17:49:07.917055",
     "exception": false,
     "start_time": "2023-10-24T17:49:07.887563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ff=compute_class_weight(class_weight=\"balanced\",classes=np.unique(labels),y=labels)\n",
    "cw=dict(zip(np.unique(labels),ff))\n",
    "\n",
    "featuress,labelss=shuffle(featuresstand,labels)\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(featuress,labelss,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d836f9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.23322547  0.13495454  1.13350448 ...  2.1002112   0.31851056\n",
      "   0.1566997 ]\n",
      " [ 0.63824366 -0.71233278 -0.39556205 ... -0.24054795  0.50560843\n",
      "   0.88806278]\n",
      " [-1.24742737  1.29715275 -1.17548073 ... -0.39592204 -1.1224069\n",
      "  -1.1806365 ]\n",
      " ...\n",
      " [ 0.10665236 -0.52924796 -0.38925088 ... -0.45893561  0.49032971\n",
      "   0.61711073]\n",
      " [ 0.69909562 -0.18717638  0.95798419 ...  0.79213681  0.73283658\n",
      "   0.64270196]\n",
      " [-0.0446162   0.12403184  0.46061598 ...  0.71612289  0.26416593\n",
      "   0.26397881]]\n"
     ]
    }
   ],
   "source": [
    "print(featuress)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dab5aeba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:07.945084Z",
     "iopub.status.busy": "2023-10-24T17:49:07.944630Z",
     "iopub.status.idle": "2023-10-24T17:49:07.951807Z",
     "shell.execute_reply": "2023-10-24T17:49:07.950811Z"
    },
    "id": "dab5aeba",
    "outputId": "daeff992-ae1f-447f-859e-3570af300035",
    "papermill": {
     "duration": 0.023502,
     "end_time": "2023-10-24T17:49:07.953515",
     "exception": false,
     "start_time": "2023-10-24T17:49:07.930013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8827, 54)\n",
      "(981, 54)\n",
      "(8827, 1)\n",
      "(981, 1)\n"
     ]
    }
   ],
   "source": [
    "xtrain=xtrain.astype(np.float32)\n",
    "xtest=xtest.astype(np.float32)\n",
    "ytrain=ytrain.astype(np.float32).to_numpy()\n",
    "ytest=ytest.astype(np.float32).to_numpy()\n",
    "\n",
    "\n",
    "ytrain=ytrain.reshape(ytrain.shape+(1,))\n",
    "ytest=ytest.reshape(ytest.shape+(1,))\n",
    "\n",
    "print(xtrain.shape)\n",
    "print(xtest.shape)\n",
    "print(ytrain.shape)\n",
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3e3cc1",
   "metadata": {
    "id": "9a3e3cc1",
    "papermill": {
     "duration": 0.012374,
     "end_time": "2023-10-24T17:49:07.979414",
     "exception": false,
     "start_time": "2023-10-24T17:49:07.967040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "af43a3ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:08.007222Z",
     "iopub.status.busy": "2023-10-24T17:49:08.005904Z",
     "iopub.status.idle": "2023-10-24T17:49:08.217785Z",
     "shell.execute_reply": "2023-10-24T17:49:08.216688Z"
    },
    "id": "af43a3ec",
    "papermill": {
     "duration": 0.228404,
     "end_time": "2023-10-24T17:49:08.220186",
     "exception": false,
     "start_time": "2023-10-24T17:49:07.991782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inpt=tf.keras.layers.Input((xtrain.shape[1]))\n",
    "d1=tf.keras.layers.Dense(256, activation='relu')(inpt)\n",
    "d1=tf.keras.layers.Dense(128, activation='relu')(d1)\n",
    "\n",
    "d2=tf.keras.layers.Dense(1,activation=\"sigmoid\")(d1)\n",
    "\n",
    "nural_network=tf.keras.Model(inputs=inpt,outputs=d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05240013",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-10-24T17:49:08.247264Z",
     "iopub.status.busy": "2023-10-24T17:49:08.246869Z",
     "iopub.status.idle": "2023-10-24T17:50:43.828014Z",
     "shell.execute_reply": "2023-10-24T17:50:43.826862Z"
    },
    "id": "05240013",
    "outputId": "12ddbf8f-d124-4675-caf8-82d0dee7b467",
    "papermill": {
     "duration": 95.597206,
     "end_time": "2023-10-24T17:50:43.829977",
     "exception": false,
     "start_time": "2023-10-24T17:49:08.232771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "69/69 [==============================] - 3s 10ms/step - loss: 0.5625 - accuracy: 0.6624 - val_loss: 0.5079 - val_accuracy: 0.7421\n",
      "Epoch 2/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.4589 - accuracy: 0.8013 - val_loss: 0.4343 - val_accuracy: 0.8206\n",
      "Epoch 3/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.3997 - accuracy: 0.8459 - val_loss: 0.3858 - val_accuracy: 0.8542\n",
      "Epoch 4/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.3609 - accuracy: 0.8617 - val_loss: 0.3544 - val_accuracy: 0.8583\n",
      "Epoch 5/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.3357 - accuracy: 0.8697 - val_loss: 0.3354 - val_accuracy: 0.8624\n",
      "Epoch 6/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.3184 - accuracy: 0.8736 - val_loss: 0.3215 - val_accuracy: 0.8654\n",
      "Epoch 7/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.3047 - accuracy: 0.8799 - val_loss: 0.3092 - val_accuracy: 0.8726\n",
      "Epoch 8/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.2944 - accuracy: 0.8826 - val_loss: 0.3002 - val_accuracy: 0.8756\n",
      "Epoch 9/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.2855 - accuracy: 0.8867 - val_loss: 0.2929 - val_accuracy: 0.8807\n",
      "Epoch 10/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.2775 - accuracy: 0.8915 - val_loss: 0.2851 - val_accuracy: 0.8797\n",
      "Epoch 11/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2705 - accuracy: 0.8945 - val_loss: 0.2791 - val_accuracy: 0.8869\n",
      "Epoch 12/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2638 - accuracy: 0.8976 - val_loss: 0.2750 - val_accuracy: 0.8940\n",
      "Epoch 13/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.2583 - accuracy: 0.9002 - val_loss: 0.2681 - val_accuracy: 0.8960\n",
      "Epoch 14/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.2527 - accuracy: 0.9023 - val_loss: 0.2642 - val_accuracy: 0.9001\n",
      "Epoch 15/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.2483 - accuracy: 0.9057 - val_loss: 0.2594 - val_accuracy: 0.8970\n",
      "Epoch 16/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.2428 - accuracy: 0.9071 - val_loss: 0.2578 - val_accuracy: 0.9001\n",
      "Epoch 17/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2385 - accuracy: 0.9091 - val_loss: 0.2525 - val_accuracy: 0.8981\n",
      "Epoch 18/300\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 0.2346 - accuracy: 0.9117 - val_loss: 0.2511 - val_accuracy: 0.8970\n",
      "Epoch 19/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2311 - accuracy: 0.9127 - val_loss: 0.2460 - val_accuracy: 0.9011\n",
      "Epoch 20/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2267 - accuracy: 0.9139 - val_loss: 0.2426 - val_accuracy: 0.9042\n",
      "Epoch 21/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.2230 - accuracy: 0.9171 - val_loss: 0.2397 - val_accuracy: 0.9052\n",
      "Epoch 22/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2192 - accuracy: 0.9176 - val_loss: 0.2376 - val_accuracy: 0.9093\n",
      "Epoch 23/300\n",
      "69/69 [==============================] - 1s 8ms/step - loss: 0.2159 - accuracy: 0.9195 - val_loss: 0.2335 - val_accuracy: 0.9093\n",
      "Epoch 24/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.2120 - accuracy: 0.9223 - val_loss: 0.2315 - val_accuracy: 0.9103\n",
      "Epoch 25/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.2093 - accuracy: 0.9232 - val_loss: 0.2286 - val_accuracy: 0.9113\n",
      "Epoch 26/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2061 - accuracy: 0.9241 - val_loss: 0.2258 - val_accuracy: 0.9103\n",
      "Epoch 27/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.2028 - accuracy: 0.9259 - val_loss: 0.2238 - val_accuracy: 0.9123\n",
      "Epoch 28/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1998 - accuracy: 0.9274 - val_loss: 0.2221 - val_accuracy: 0.9154\n",
      "Epoch 29/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.1966 - accuracy: 0.9281 - val_loss: 0.2194 - val_accuracy: 0.9144\n",
      "Epoch 30/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.1934 - accuracy: 0.9295 - val_loss: 0.2166 - val_accuracy: 0.9154\n",
      "Epoch 31/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.1916 - accuracy: 0.9311 - val_loss: 0.2137 - val_accuracy: 0.9174\n",
      "Epoch 32/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1888 - accuracy: 0.9311 - val_loss: 0.2124 - val_accuracy: 0.9134\n",
      "Epoch 33/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.1850 - accuracy: 0.9346 - val_loss: 0.2089 - val_accuracy: 0.9154\n",
      "Epoch 34/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.1823 - accuracy: 0.9344 - val_loss: 0.2074 - val_accuracy: 0.9154\n",
      "Epoch 35/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.1795 - accuracy: 0.9377 - val_loss: 0.2067 - val_accuracy: 0.9154\n",
      "Epoch 36/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.1776 - accuracy: 0.9353 - val_loss: 0.2022 - val_accuracy: 0.9195\n",
      "Epoch 37/300\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 0.1744 - accuracy: 0.9379 - val_loss: 0.2005 - val_accuracy: 0.9185\n",
      "Epoch 38/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.1720 - accuracy: 0.9392 - val_loss: 0.1987 - val_accuracy: 0.9154\n",
      "Epoch 39/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1698 - accuracy: 0.9409 - val_loss: 0.2005 - val_accuracy: 0.9174\n",
      "Epoch 40/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1680 - accuracy: 0.9413 - val_loss: 0.1955 - val_accuracy: 0.9215\n",
      "Epoch 41/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.1647 - accuracy: 0.9435 - val_loss: 0.1952 - val_accuracy: 0.9174\n",
      "Epoch 42/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.1634 - accuracy: 0.9426 - val_loss: 0.1929 - val_accuracy: 0.9195\n",
      "Epoch 43/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1611 - accuracy: 0.9451 - val_loss: 0.1912 - val_accuracy: 0.9235\n",
      "Epoch 44/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1591 - accuracy: 0.9468 - val_loss: 0.1875 - val_accuracy: 0.9246\n",
      "Epoch 45/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1563 - accuracy: 0.9480 - val_loss: 0.1892 - val_accuracy: 0.9225\n",
      "Epoch 46/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1544 - accuracy: 0.9470 - val_loss: 0.1850 - val_accuracy: 0.9205\n",
      "Epoch 47/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.1517 - accuracy: 0.9504 - val_loss: 0.1839 - val_accuracy: 0.9266\n",
      "Epoch 48/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1499 - accuracy: 0.9497 - val_loss: 0.1824 - val_accuracy: 0.9246\n",
      "Epoch 49/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1486 - accuracy: 0.9504 - val_loss: 0.1819 - val_accuracy: 0.9246\n",
      "Epoch 50/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1463 - accuracy: 0.9499 - val_loss: 0.1785 - val_accuracy: 0.9276\n",
      "Epoch 51/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1442 - accuracy: 0.9508 - val_loss: 0.1761 - val_accuracy: 0.9317\n",
      "Epoch 52/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1421 - accuracy: 0.9520 - val_loss: 0.1748 - val_accuracy: 0.9286\n",
      "Epoch 53/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1403 - accuracy: 0.9531 - val_loss: 0.1729 - val_accuracy: 0.9297\n",
      "Epoch 54/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1387 - accuracy: 0.9537 - val_loss: 0.1738 - val_accuracy: 0.9307\n",
      "Epoch 55/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1378 - accuracy: 0.9539 - val_loss: 0.1719 - val_accuracy: 0.9297\n",
      "Epoch 56/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1353 - accuracy: 0.9548 - val_loss: 0.1699 - val_accuracy: 0.9297\n",
      "Epoch 57/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1333 - accuracy: 0.9550 - val_loss: 0.1675 - val_accuracy: 0.9348\n",
      "Epoch 58/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.1320 - accuracy: 0.9559 - val_loss: 0.1655 - val_accuracy: 0.9348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1303 - accuracy: 0.9567 - val_loss: 0.1656 - val_accuracy: 0.9317\n",
      "Epoch 60/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1286 - accuracy: 0.9577 - val_loss: 0.1659 - val_accuracy: 0.9317\n",
      "Epoch 61/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1273 - accuracy: 0.9580 - val_loss: 0.1626 - val_accuracy: 0.9337\n",
      "Epoch 62/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1255 - accuracy: 0.9573 - val_loss: 0.1642 - val_accuracy: 0.9348\n",
      "Epoch 63/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1252 - accuracy: 0.9575 - val_loss: 0.1614 - val_accuracy: 0.9348\n",
      "Epoch 64/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1232 - accuracy: 0.9605 - val_loss: 0.1622 - val_accuracy: 0.9368\n",
      "Epoch 65/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.1222 - accuracy: 0.9584 - val_loss: 0.1590 - val_accuracy: 0.9378\n",
      "Epoch 66/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1206 - accuracy: 0.9592 - val_loss: 0.1581 - val_accuracy: 0.9337\n",
      "Epoch 67/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1188 - accuracy: 0.9608 - val_loss: 0.1575 - val_accuracy: 0.9378\n",
      "Epoch 68/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1170 - accuracy: 0.9615 - val_loss: 0.1549 - val_accuracy: 0.9368\n",
      "Epoch 69/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1158 - accuracy: 0.9623 - val_loss: 0.1560 - val_accuracy: 0.9378\n",
      "Epoch 70/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1143 - accuracy: 0.9627 - val_loss: 0.1530 - val_accuracy: 0.9368\n",
      "Epoch 71/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1139 - accuracy: 0.9623 - val_loss: 0.1527 - val_accuracy: 0.9368\n",
      "Epoch 72/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1122 - accuracy: 0.9634 - val_loss: 0.1506 - val_accuracy: 0.9409\n",
      "Epoch 73/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1108 - accuracy: 0.9633 - val_loss: 0.1503 - val_accuracy: 0.9399\n",
      "Epoch 74/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1093 - accuracy: 0.9640 - val_loss: 0.1496 - val_accuracy: 0.9409\n",
      "Epoch 75/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1079 - accuracy: 0.9652 - val_loss: 0.1499 - val_accuracy: 0.9399\n",
      "Epoch 76/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1071 - accuracy: 0.9656 - val_loss: 0.1464 - val_accuracy: 0.9399\n",
      "Epoch 77/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1056 - accuracy: 0.9665 - val_loss: 0.1479 - val_accuracy: 0.9419\n",
      "Epoch 78/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1049 - accuracy: 0.9665 - val_loss: 0.1490 - val_accuracy: 0.9388\n",
      "Epoch 79/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1039 - accuracy: 0.9654 - val_loss: 0.1452 - val_accuracy: 0.9388\n",
      "Epoch 80/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.1026 - accuracy: 0.9669 - val_loss: 0.1441 - val_accuracy: 0.9419\n",
      "Epoch 81/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.1010 - accuracy: 0.9665 - val_loss: 0.1433 - val_accuracy: 0.9419\n",
      "Epoch 82/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0996 - accuracy: 0.9676 - val_loss: 0.1435 - val_accuracy: 0.9419\n",
      "Epoch 83/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0990 - accuracy: 0.9686 - val_loss: 0.1457 - val_accuracy: 0.9358\n",
      "Epoch 84/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0988 - accuracy: 0.9696 - val_loss: 0.1393 - val_accuracy: 0.9429\n",
      "Epoch 85/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0965 - accuracy: 0.9700 - val_loss: 0.1407 - val_accuracy: 0.9399\n",
      "Epoch 86/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0954 - accuracy: 0.9691 - val_loss: 0.1414 - val_accuracy: 0.9399\n",
      "Epoch 87/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0948 - accuracy: 0.9698 - val_loss: 0.1427 - val_accuracy: 0.9368\n",
      "Epoch 88/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0935 - accuracy: 0.9708 - val_loss: 0.1387 - val_accuracy: 0.9409\n",
      "Epoch 89/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0929 - accuracy: 0.9716 - val_loss: 0.1376 - val_accuracy: 0.9450\n",
      "Epoch 90/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0925 - accuracy: 0.9717 - val_loss: 0.1370 - val_accuracy: 0.9429\n",
      "Epoch 91/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0904 - accuracy: 0.9710 - val_loss: 0.1351 - val_accuracy: 0.9450\n",
      "Epoch 92/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0897 - accuracy: 0.9715 - val_loss: 0.1375 - val_accuracy: 0.9439\n",
      "Epoch 93/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0883 - accuracy: 0.9736 - val_loss: 0.1364 - val_accuracy: 0.9490\n",
      "Epoch 94/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0885 - accuracy: 0.9715 - val_loss: 0.1338 - val_accuracy: 0.9501\n",
      "Epoch 95/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0863 - accuracy: 0.9730 - val_loss: 0.1332 - val_accuracy: 0.9450\n",
      "Epoch 96/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0853 - accuracy: 0.9745 - val_loss: 0.1382 - val_accuracy: 0.9429\n",
      "Epoch 97/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0853 - accuracy: 0.9746 - val_loss: 0.1316 - val_accuracy: 0.9480\n",
      "Epoch 98/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0849 - accuracy: 0.9746 - val_loss: 0.1348 - val_accuracy: 0.9460\n",
      "Epoch 99/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0827 - accuracy: 0.9742 - val_loss: 0.1295 - val_accuracy: 0.9450\n",
      "Epoch 100/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0814 - accuracy: 0.9762 - val_loss: 0.1310 - val_accuracy: 0.9460\n",
      "Epoch 101/300\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 0.0811 - accuracy: 0.9761 - val_loss: 0.1404 - val_accuracy: 0.9470\n",
      "Epoch 102/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0804 - accuracy: 0.9756 - val_loss: 0.1307 - val_accuracy: 0.9521\n",
      "Epoch 103/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0791 - accuracy: 0.9771 - val_loss: 0.1308 - val_accuracy: 0.9439\n",
      "Epoch 104/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0779 - accuracy: 0.9772 - val_loss: 0.1303 - val_accuracy: 0.9501\n",
      "Epoch 105/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0770 - accuracy: 0.9779 - val_loss: 0.1260 - val_accuracy: 0.9521\n",
      "Epoch 106/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0764 - accuracy: 0.9779 - val_loss: 0.1283 - val_accuracy: 0.9439\n",
      "Epoch 107/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.0761 - accuracy: 0.9780 - val_loss: 0.1278 - val_accuracy: 0.9511\n",
      "Epoch 108/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.0744 - accuracy: 0.9789 - val_loss: 0.1298 - val_accuracy: 0.9439\n",
      "Epoch 109/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0740 - accuracy: 0.9779 - val_loss: 0.1259 - val_accuracy: 0.9490\n",
      "Epoch 110/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0751 - accuracy: 0.9794 - val_loss: 0.1262 - val_accuracy: 0.9460\n",
      "Epoch 111/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0724 - accuracy: 0.9795 - val_loss: 0.1246 - val_accuracy: 0.9511\n",
      "Epoch 112/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0718 - accuracy: 0.9798 - val_loss: 0.1286 - val_accuracy: 0.9450\n",
      "Epoch 113/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.0704 - accuracy: 0.9806 - val_loss: 0.1252 - val_accuracy: 0.9490\n",
      "Epoch 114/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0691 - accuracy: 0.9813 - val_loss: 0.1246 - val_accuracy: 0.9531\n",
      "Epoch 115/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0687 - accuracy: 0.9804 - val_loss: 0.1214 - val_accuracy: 0.9572\n",
      "Epoch 116/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0683 - accuracy: 0.9814 - val_loss: 0.1304 - val_accuracy: 0.9501\n",
      "Epoch 117/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0681 - accuracy: 0.9818 - val_loss: 0.1268 - val_accuracy: 0.9480\n",
      "Epoch 118/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.0670 - accuracy: 0.9812 - val_loss: 0.1260 - val_accuracy: 0.9470\n",
      "Epoch 119/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0657 - accuracy: 0.9824 - val_loss: 0.1245 - val_accuracy: 0.9511\n",
      "Epoch 120/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0647 - accuracy: 0.9829 - val_loss: 0.1268 - val_accuracy: 0.9480\n",
      "Epoch 121/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0646 - accuracy: 0.9822 - val_loss: 0.1281 - val_accuracy: 0.9521\n",
      "Epoch 122/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0639 - accuracy: 0.9826 - val_loss: 0.1227 - val_accuracy: 0.9501\n",
      "Epoch 123/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0634 - accuracy: 0.9830 - val_loss: 0.1264 - val_accuracy: 0.9562\n",
      "Epoch 124/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0623 - accuracy: 0.9835 - val_loss: 0.1206 - val_accuracy: 0.9562\n",
      "Epoch 125/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0617 - accuracy: 0.9830 - val_loss: 0.1184 - val_accuracy: 0.9572\n",
      "Epoch 126/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0607 - accuracy: 0.9829 - val_loss: 0.1203 - val_accuracy: 0.9562\n",
      "Epoch 127/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0596 - accuracy: 0.9841 - val_loss: 0.1205 - val_accuracy: 0.9562\n",
      "Epoch 128/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0590 - accuracy: 0.9839 - val_loss: 0.1224 - val_accuracy: 0.9541\n",
      "Epoch 129/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0584 - accuracy: 0.9847 - val_loss: 0.1186 - val_accuracy: 0.9551\n",
      "Epoch 130/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0585 - accuracy: 0.9840 - val_loss: 0.1152 - val_accuracy: 0.9592\n",
      "Epoch 131/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0569 - accuracy: 0.9854 - val_loss: 0.1192 - val_accuracy: 0.9562\n",
      "Epoch 132/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0564 - accuracy: 0.9849 - val_loss: 0.1207 - val_accuracy: 0.9562\n",
      "Epoch 133/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0558 - accuracy: 0.9854 - val_loss: 0.1254 - val_accuracy: 0.9531\n",
      "Epoch 134/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0552 - accuracy: 0.9854 - val_loss: 0.1227 - val_accuracy: 0.9572\n",
      "Epoch 135/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0547 - accuracy: 0.9857 - val_loss: 0.1185 - val_accuracy: 0.9572\n",
      "Epoch 136/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0543 - accuracy: 0.9865 - val_loss: 0.1177 - val_accuracy: 0.9551\n",
      "Epoch 137/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0527 - accuracy: 0.9865 - val_loss: 0.1191 - val_accuracy: 0.9541\n",
      "Epoch 138/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0527 - accuracy: 0.9860 - val_loss: 0.1212 - val_accuracy: 0.9551\n",
      "Epoch 139/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0518 - accuracy: 0.9862 - val_loss: 0.1192 - val_accuracy: 0.9623\n",
      "Epoch 140/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0515 - accuracy: 0.9865 - val_loss: 0.1184 - val_accuracy: 0.9592\n",
      "Epoch 141/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0510 - accuracy: 0.9860 - val_loss: 0.1161 - val_accuracy: 0.9562\n",
      "Epoch 142/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0497 - accuracy: 0.9869 - val_loss: 0.1218 - val_accuracy: 0.9592\n",
      "Epoch 143/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0493 - accuracy: 0.9870 - val_loss: 0.1153 - val_accuracy: 0.9592\n",
      "Epoch 144/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0493 - accuracy: 0.9869 - val_loss: 0.1191 - val_accuracy: 0.9613\n",
      "Epoch 145/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0484 - accuracy: 0.9874 - val_loss: 0.1209 - val_accuracy: 0.9572\n",
      "Epoch 146/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0474 - accuracy: 0.9872 - val_loss: 0.1170 - val_accuracy: 0.9582\n",
      "Epoch 147/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0466 - accuracy: 0.9879 - val_loss: 0.1167 - val_accuracy: 0.9592\n",
      "Epoch 148/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0463 - accuracy: 0.9883 - val_loss: 0.1150 - val_accuracy: 0.9613\n",
      "Epoch 149/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0462 - accuracy: 0.9869 - val_loss: 0.1159 - val_accuracy: 0.9592\n",
      "Epoch 150/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0453 - accuracy: 0.9879 - val_loss: 0.1171 - val_accuracy: 0.9582\n",
      "Epoch 151/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0452 - accuracy: 0.9878 - val_loss: 0.1172 - val_accuracy: 0.9602\n",
      "Epoch 152/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0436 - accuracy: 0.9895 - val_loss: 0.1157 - val_accuracy: 0.9602\n",
      "Epoch 153/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0436 - accuracy: 0.9883 - val_loss: 0.1261 - val_accuracy: 0.9602\n",
      "Epoch 154/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0433 - accuracy: 0.9895 - val_loss: 0.1158 - val_accuracy: 0.9643\n",
      "Epoch 155/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0427 - accuracy: 0.9888 - val_loss: 0.1140 - val_accuracy: 0.9592\n",
      "Epoch 156/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0423 - accuracy: 0.9881 - val_loss: 0.1140 - val_accuracy: 0.9592\n",
      "Epoch 157/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0414 - accuracy: 0.9896 - val_loss: 0.1219 - val_accuracy: 0.9551\n",
      "Epoch 158/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0411 - accuracy: 0.9891 - val_loss: 0.1159 - val_accuracy: 0.9613\n",
      "Epoch 159/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0400 - accuracy: 0.9901 - val_loss: 0.1158 - val_accuracy: 0.9602\n",
      "Epoch 160/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0398 - accuracy: 0.9904 - val_loss: 0.1144 - val_accuracy: 0.9613\n",
      "Epoch 161/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0388 - accuracy: 0.9905 - val_loss: 0.1167 - val_accuracy: 0.9602\n",
      "Epoch 162/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0388 - accuracy: 0.9907 - val_loss: 0.1221 - val_accuracy: 0.9602\n",
      "Epoch 163/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0386 - accuracy: 0.9903 - val_loss: 0.1147 - val_accuracy: 0.9623\n",
      "Epoch 164/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.0375 - accuracy: 0.9911 - val_loss: 0.1192 - val_accuracy: 0.9572\n",
      "Epoch 165/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0369 - accuracy: 0.9913 - val_loss: 0.1162 - val_accuracy: 0.9592\n",
      "Epoch 166/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0367 - accuracy: 0.9909 - val_loss: 0.1218 - val_accuracy: 0.9592\n",
      "Epoch 167/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0365 - accuracy: 0.9909 - val_loss: 0.1147 - val_accuracy: 0.9602\n",
      "Epoch 168/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0360 - accuracy: 0.9911 - val_loss: 0.1200 - val_accuracy: 0.9582\n",
      "Epoch 169/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0358 - accuracy: 0.9913 - val_loss: 0.1202 - val_accuracy: 0.9562\n",
      "Epoch 170/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0346 - accuracy: 0.9915 - val_loss: 0.1119 - val_accuracy: 0.9653\n",
      "Epoch 171/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0349 - accuracy: 0.9918 - val_loss: 0.1130 - val_accuracy: 0.9613\n",
      "Epoch 172/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0338 - accuracy: 0.9921 - val_loss: 0.1197 - val_accuracy: 0.9562\n",
      "Epoch 173/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0334 - accuracy: 0.9925 - val_loss: 0.1157 - val_accuracy: 0.9582\n",
      "Epoch 174/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0335 - accuracy: 0.9921 - val_loss: 0.1155 - val_accuracy: 0.9623\n",
      "Epoch 175/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0326 - accuracy: 0.9931 - val_loss: 0.1150 - val_accuracy: 0.9613\n",
      "Epoch 176/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0325 - accuracy: 0.9920 - val_loss: 0.1136 - val_accuracy: 0.9633\n",
      "Epoch 177/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0321 - accuracy: 0.9924 - val_loss: 0.1168 - val_accuracy: 0.9623\n",
      "Epoch 178/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0315 - accuracy: 0.9925 - val_loss: 0.1189 - val_accuracy: 0.9602\n",
      "Epoch 179/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0308 - accuracy: 0.9924 - val_loss: 0.1178 - val_accuracy: 0.9582\n",
      "Epoch 180/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0317 - accuracy: 0.9923 - val_loss: 0.1160 - val_accuracy: 0.9623\n",
      "Epoch 181/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0303 - accuracy: 0.9931 - val_loss: 0.1277 - val_accuracy: 0.9592\n",
      "Epoch 182/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0297 - accuracy: 0.9929 - val_loss: 0.1210 - val_accuracy: 0.9613\n",
      "Epoch 183/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0295 - accuracy: 0.9935 - val_loss: 0.1148 - val_accuracy: 0.9633\n",
      "Epoch 184/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0285 - accuracy: 0.9940 - val_loss: 0.1235 - val_accuracy: 0.9613\n",
      "Epoch 185/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0283 - accuracy: 0.9935 - val_loss: 0.1186 - val_accuracy: 0.9623\n",
      "Epoch 186/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0284 - accuracy: 0.9930 - val_loss: 0.1117 - val_accuracy: 0.9643\n",
      "Epoch 187/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0282 - accuracy: 0.9938 - val_loss: 0.1191 - val_accuracy: 0.9582\n",
      "Epoch 188/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0276 - accuracy: 0.9934 - val_loss: 0.1320 - val_accuracy: 0.9613\n",
      "Epoch 189/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0275 - accuracy: 0.9931 - val_loss: 0.1135 - val_accuracy: 0.9633\n",
      "Epoch 190/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0268 - accuracy: 0.9938 - val_loss: 0.1225 - val_accuracy: 0.9592\n",
      "Epoch 191/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0262 - accuracy: 0.9937 - val_loss: 0.1200 - val_accuracy: 0.9633\n",
      "Epoch 192/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0259 - accuracy: 0.9942 - val_loss: 0.1214 - val_accuracy: 0.9633\n",
      "Epoch 193/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0256 - accuracy: 0.9942 - val_loss: 0.1215 - val_accuracy: 0.9643\n",
      "Epoch 194/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0250 - accuracy: 0.9946 - val_loss: 0.1196 - val_accuracy: 0.9602\n",
      "Epoch 195/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0247 - accuracy: 0.9939 - val_loss: 0.1205 - val_accuracy: 0.9623\n",
      "Epoch 196/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0247 - accuracy: 0.9946 - val_loss: 0.1188 - val_accuracy: 0.9633\n",
      "Epoch 197/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 0.9942 - val_loss: 0.1185 - val_accuracy: 0.9643\n",
      "Epoch 198/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0239 - accuracy: 0.9947 - val_loss: 0.1231 - val_accuracy: 0.9643\n",
      "Epoch 199/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0235 - accuracy: 0.9943 - val_loss: 0.1164 - val_accuracy: 0.9664\n",
      "Epoch 200/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0233 - accuracy: 0.9954 - val_loss: 0.1237 - val_accuracy: 0.9623\n",
      "Epoch 201/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0229 - accuracy: 0.9948 - val_loss: 0.1242 - val_accuracy: 0.9633\n",
      "Epoch 202/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0225 - accuracy: 0.9951 - val_loss: 0.1249 - val_accuracy: 0.9633\n",
      "Epoch 203/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0222 - accuracy: 0.9951 - val_loss: 0.1258 - val_accuracy: 0.9582\n",
      "Epoch 204/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0221 - accuracy: 0.9954 - val_loss: 0.1235 - val_accuracy: 0.9633\n",
      "Epoch 205/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0219 - accuracy: 0.9952 - val_loss: 0.1156 - val_accuracy: 0.9653\n",
      "Epoch 206/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.9956 - val_loss: 0.1172 - val_accuracy: 0.9653\n",
      "Epoch 207/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.9956 - val_loss: 0.1344 - val_accuracy: 0.9633\n",
      "Epoch 208/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0209 - accuracy: 0.9950 - val_loss: 0.1231 - val_accuracy: 0.9623\n",
      "Epoch 209/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 0.9958 - val_loss: 0.1295 - val_accuracy: 0.9633\n",
      "Epoch 210/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.9958 - val_loss: 0.1309 - val_accuracy: 0.9582\n",
      "Epoch 211/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0198 - accuracy: 0.9956 - val_loss: 0.1189 - val_accuracy: 0.9674\n",
      "Epoch 212/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0194 - accuracy: 0.9960 - val_loss: 0.1224 - val_accuracy: 0.9623\n",
      "Epoch 213/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0189 - accuracy: 0.9964 - val_loss: 0.1249 - val_accuracy: 0.9653\n",
      "Epoch 214/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.0188 - accuracy: 0.9960 - val_loss: 0.1237 - val_accuracy: 0.9664\n",
      "Epoch 215/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0180 - accuracy: 0.9963 - val_loss: 0.1258 - val_accuracy: 0.9664\n",
      "Epoch 216/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0182 - accuracy: 0.9961 - val_loss: 0.1207 - val_accuracy: 0.9653\n",
      "Epoch 217/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 0.9966 - val_loss: 0.1279 - val_accuracy: 0.9602\n",
      "Epoch 218/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0185 - accuracy: 0.9965 - val_loss: 0.1270 - val_accuracy: 0.9643\n",
      "Epoch 219/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0177 - accuracy: 0.9965 - val_loss: 0.1340 - val_accuracy: 0.9613\n",
      "Epoch 220/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0176 - accuracy: 0.9964 - val_loss: 0.1191 - val_accuracy: 0.9653\n",
      "Epoch 221/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 0.9966 - val_loss: 0.1373 - val_accuracy: 0.9613\n",
      "Epoch 222/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0176 - accuracy: 0.9961 - val_loss: 0.1257 - val_accuracy: 0.9643\n",
      "Epoch 223/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0164 - accuracy: 0.9966 - val_loss: 0.1395 - val_accuracy: 0.9633\n",
      "Epoch 224/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0166 - accuracy: 0.9968 - val_loss: 0.1373 - val_accuracy: 0.9643\n",
      "Epoch 225/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0163 - accuracy: 0.9965 - val_loss: 0.1351 - val_accuracy: 0.9592\n",
      "Epoch 226/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0158 - accuracy: 0.9967 - val_loss: 0.1226 - val_accuracy: 0.9643\n",
      "Epoch 227/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0154 - accuracy: 0.9968 - val_loss: 0.1268 - val_accuracy: 0.9653\n",
      "Epoch 228/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0149 - accuracy: 0.9969 - val_loss: 0.1236 - val_accuracy: 0.9674\n",
      "Epoch 229/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0154 - accuracy: 0.9969 - val_loss: 0.1325 - val_accuracy: 0.9643\n",
      "Epoch 230/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0146 - accuracy: 0.9969 - val_loss: 0.1317 - val_accuracy: 0.9643\n",
      "Epoch 231/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0145 - accuracy: 0.9973 - val_loss: 0.1246 - val_accuracy: 0.9653\n",
      "Epoch 232/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0143 - accuracy: 0.9973 - val_loss: 0.1382 - val_accuracy: 0.9613\n",
      "Epoch 233/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0140 - accuracy: 0.9976 - val_loss: 0.1260 - val_accuracy: 0.9684\n",
      "Epoch 234/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0139 - accuracy: 0.9971 - val_loss: 0.1365 - val_accuracy: 0.9623\n",
      "Epoch 235/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 0.9977 - val_loss: 0.1264 - val_accuracy: 0.9684\n",
      "Epoch 236/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0136 - accuracy: 0.9977 - val_loss: 0.1273 - val_accuracy: 0.9674\n",
      "Epoch 237/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0136 - accuracy: 0.9977 - val_loss: 0.1268 - val_accuracy: 0.9613\n",
      "Epoch 238/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 0.9977 - val_loss: 0.1446 - val_accuracy: 0.9562\n",
      "Epoch 239/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 0.9975 - val_loss: 0.1254 - val_accuracy: 0.9684\n",
      "Epoch 240/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 0.9974 - val_loss: 0.1353 - val_accuracy: 0.9643\n",
      "Epoch 241/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0124 - accuracy: 0.9981 - val_loss: 0.1353 - val_accuracy: 0.9653\n",
      "Epoch 242/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0126 - accuracy: 0.9981 - val_loss: 0.1341 - val_accuracy: 0.9623\n",
      "Epoch 243/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0128 - accuracy: 0.9978 - val_loss: 0.1231 - val_accuracy: 0.9664\n",
      "Epoch 244/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0122 - accuracy: 0.9977 - val_loss: 0.1347 - val_accuracy: 0.9664\n",
      "Epoch 245/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0118 - accuracy: 0.9984 - val_loss: 0.1393 - val_accuracy: 0.9674\n",
      "Epoch 246/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0116 - accuracy: 0.9981 - val_loss: 0.1310 - val_accuracy: 0.9664\n",
      "Epoch 247/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0116 - accuracy: 0.9982 - val_loss: 0.1385 - val_accuracy: 0.9674\n",
      "Epoch 248/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.9983 - val_loss: 0.1406 - val_accuracy: 0.9664\n",
      "Epoch 249/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0109 - accuracy: 0.9981 - val_loss: 0.1393 - val_accuracy: 0.9653\n",
      "Epoch 250/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0108 - accuracy: 0.9984 - val_loss: 0.1414 - val_accuracy: 0.9633\n",
      "Epoch 251/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0109 - accuracy: 0.9988 - val_loss: 0.1434 - val_accuracy: 0.9684\n",
      "Epoch 252/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0103 - accuracy: 0.9984 - val_loss: 0.1396 - val_accuracy: 0.9674\n",
      "Epoch 253/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0112 - accuracy: 0.9982 - val_loss: 0.1397 - val_accuracy: 0.9633\n",
      "Epoch 254/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9984 - val_loss: 0.1326 - val_accuracy: 0.9664\n",
      "Epoch 255/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0098 - accuracy: 0.9988 - val_loss: 0.1433 - val_accuracy: 0.9653\n",
      "Epoch 256/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.9986 - val_loss: 0.1472 - val_accuracy: 0.9592\n",
      "Epoch 257/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.9983 - val_loss: 0.1401 - val_accuracy: 0.9653\n",
      "Epoch 258/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.9988 - val_loss: 0.1452 - val_accuracy: 0.9643\n",
      "Epoch 259/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0093 - accuracy: 0.9986 - val_loss: 0.1515 - val_accuracy: 0.9613\n",
      "Epoch 260/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0138 - accuracy: 0.9978 - val_loss: 0.1721 - val_accuracy: 0.9653\n",
      "Epoch 261/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.1603 - val_accuracy: 0.9653\n",
      "Epoch 262/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 0.9990 - val_loss: 0.1381 - val_accuracy: 0.9684\n",
      "Epoch 263/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0090 - accuracy: 0.9991 - val_loss: 0.1436 - val_accuracy: 0.9602\n",
      "Epoch 264/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0090 - accuracy: 0.9986 - val_loss: 0.1352 - val_accuracy: 0.9704\n",
      "Epoch 265/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 0.9988 - val_loss: 0.1491 - val_accuracy: 0.9664\n",
      "Epoch 266/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0088 - accuracy: 0.9988 - val_loss: 0.1483 - val_accuracy: 0.9653\n",
      "Epoch 267/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0085 - accuracy: 0.9988 - val_loss: 0.1526 - val_accuracy: 0.9664\n",
      "Epoch 268/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.0080 - accuracy: 0.9986 - val_loss: 0.1573 - val_accuracy: 0.9633\n",
      "Epoch 269/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0083 - accuracy: 0.9990 - val_loss: 0.1404 - val_accuracy: 0.9694\n",
      "Epoch 270/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.0083 - accuracy: 0.9988 - val_loss: 0.1488 - val_accuracy: 0.9633\n",
      "Epoch 271/300\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.0078 - accuracy: 0.9988 - val_loss: 0.1454 - val_accuracy: 0.9633\n",
      "Epoch 272/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.1525 - val_accuracy: 0.9664\n",
      "Epoch 273/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0076 - accuracy: 0.9989 - val_loss: 0.1530 - val_accuracy: 0.9653\n",
      "Epoch 274/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0074 - accuracy: 0.9991 - val_loss: 0.1507 - val_accuracy: 0.9664\n",
      "Epoch 275/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0074 - accuracy: 0.9989 - val_loss: 0.1578 - val_accuracy: 0.9653\n",
      "Epoch 276/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0073 - accuracy: 0.9990 - val_loss: 0.1490 - val_accuracy: 0.9664\n",
      "Epoch 277/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0074 - accuracy: 0.9992 - val_loss: 0.1547 - val_accuracy: 0.9674\n",
      "Epoch 278/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.9989 - val_loss: 0.1572 - val_accuracy: 0.9643\n",
      "Epoch 279/300\n",
      "69/69 [==============================] - 0s 7ms/step - loss: 0.0068 - accuracy: 0.9993 - val_loss: 0.1494 - val_accuracy: 0.9674\n",
      "Epoch 280/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0069 - accuracy: 0.9990 - val_loss: 0.1510 - val_accuracy: 0.9653\n",
      "Epoch 281/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0065 - accuracy: 0.9992 - val_loss: 0.1553 - val_accuracy: 0.9643\n",
      "Epoch 282/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0067 - accuracy: 0.9990 - val_loss: 0.1502 - val_accuracy: 0.9664\n",
      "Epoch 283/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0064 - accuracy: 0.9991 - val_loss: 0.1480 - val_accuracy: 0.9674\n",
      "Epoch 284/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 0.9992 - val_loss: 0.1656 - val_accuracy: 0.9623\n",
      "Epoch 285/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.9994 - val_loss: 0.1510 - val_accuracy: 0.9643\n",
      "Epoch 286/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0062 - accuracy: 0.9991 - val_loss: 0.1573 - val_accuracy: 0.9653\n",
      "Epoch 287/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 0s 6ms/step - loss: 0.0064 - accuracy: 0.9989 - val_loss: 0.1558 - val_accuracy: 0.9653\n",
      "Epoch 288/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0059 - accuracy: 0.9992 - val_loss: 0.1654 - val_accuracy: 0.9623\n",
      "Epoch 289/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0057 - accuracy: 0.9993 - val_loss: 0.1524 - val_accuracy: 0.9664\n",
      "Epoch 290/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0057 - accuracy: 0.9993 - val_loss: 0.1524 - val_accuracy: 0.9684\n",
      "Epoch 291/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0055 - accuracy: 0.9995 - val_loss: 0.1557 - val_accuracy: 0.9653\n",
      "Epoch 292/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0058 - accuracy: 0.9990 - val_loss: 0.1690 - val_accuracy: 0.9643\n",
      "Epoch 293/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 0.9993 - val_loss: 0.1613 - val_accuracy: 0.9613\n",
      "Epoch 294/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0055 - accuracy: 0.9992 - val_loss: 0.1567 - val_accuracy: 0.9674\n",
      "Epoch 295/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0052 - accuracy: 0.9995 - val_loss: 0.1497 - val_accuracy: 0.9715\n",
      "Epoch 296/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0052 - accuracy: 0.9997 - val_loss: 0.1599 - val_accuracy: 0.9664\n",
      "Epoch 297/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0051 - accuracy: 0.9992 - val_loss: 0.1654 - val_accuracy: 0.9674\n",
      "Epoch 298/300\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0051 - accuracy: 0.9994 - val_loss: 0.1643 - val_accuracy: 0.9674\n",
      "Epoch 299/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0046 - accuracy: 0.9995 - val_loss: 0.1658 - val_accuracy: 0.9602\n",
      "Epoch 300/300\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 0.9994 - val_loss: 0.1680 - val_accuracy: 0.9643\n"
     ]
    }
   ],
   "source": [
    "nural_network.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                      loss='binary_crossentropy'\n",
    "                    ,metrics=[\"accuracy\"])\n",
    "\n",
    "history_combined=nural_network.fit(xtrain,ytrain,validation_data=(xtest,ytest),batch_size=128,epochs=300,\n",
    "                                  class_weight=cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07S2Kn_7pqk4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07S2Kn_7pqk4",
    "outputId": "64a362d6-ae72-4eac-e498-7608c6083eda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "ytt=nural_network.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1WWOW6sMw5FW",
   "metadata": {
    "id": "1WWOW6sMw5FW"
   },
   "outputs": [],
   "source": [
    "# this will change number's scientific notation to normal float\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k9sdqAXmqDm1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9sdqAXmqDm1",
    "outputId": "43ded529-de1b-46d4-8bdb-581dc77648fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: [[0.000000]\n",
      " [0.996185]\n",
      " [0.999997]\n",
      " [0.000000]\n",
      " [0.000000]\n",
      " [0.999542]\n",
      " [0.999995]\n",
      " [0.000021]\n",
      " [0.999999]\n",
      " [0.999924]]\n",
      "Probabilities (rounded): [[0.000000]\n",
      " [1.000000]\n",
      " [1.000000]\n",
      " [0.000000]\n",
      " [0.000000]\n",
      " [1.000000]\n",
      " [1.000000]\n",
      " [0.000000]\n",
      " [1.000000]\n",
      " [1.000000]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Probabilities: {ytt[:10]}\")\n",
    "print(f\"Probabilities (rounded): {np.round(ytt)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f891d6f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-10-24T17:50:44.116506Z",
     "iopub.status.busy": "2023-10-24T17:50:44.116105Z",
     "iopub.status.idle": "2023-10-24T17:50:44.336880Z",
     "shell.execute_reply": "2023-10-24T17:50:44.335113Z"
    },
    "id": "f891d6f9",
    "outputId": "fece4979-a425-471c-8f06-33e30f45a412",
    "papermill": {
     "duration": 0.366874,
     "end_time": "2023-10-24T17:50:44.340112",
     "exception": false,
     "start_time": "2023-10-24T17:50:43.973238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.93      0.96       480\n",
      "         1.0       0.94      0.99      0.97       501\n",
      "\n",
      "    accuracy                           0.96       981\n",
      "   macro avg       0.97      0.96      0.96       981\n",
      "weighted avg       0.97      0.96      0.96       981\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ytt=nural_network.predict(xtest)\n",
    "dd=np.round(ytt)\n",
    "print(classification_report(ytest,dd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PO1HTqWmCnvi",
   "metadata": {
    "id": "PO1HTqWmCnvi"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ec897e6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "ec897e6a",
    "outputId": "eb9e420b-c8f7-4e00-95f4-eb8f140cffff",
    "papermill": {
     "duration": 0.14692,
     "end_time": "2023-10-24T17:51:39.252935",
     "exception": false,
     "start_time": "2023-10-24T17:51:39.106015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130f8371",
   "metadata": {
    "id": "130f8371",
    "papermill": {
     "duration": 0.147149,
     "end_time": "2023-10-24T17:51:39.547251",
     "exception": false,
     "start_time": "2023-10-24T17:51:39.400102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = lr.predict(xtest)\n",
    "predictions_probability = lr.predict_proba(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x7Rgq1VvC5iE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7Rgq1VvC5iE",
    "outputId": "de8b719d-678e-424c-b5ab-ce2d19510ce5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.000000, 1.000000, 1.000000, 0.000000, 0.000000], dtype=float32),\n",
       " array([[0.950287, 0.049713],\n",
       "        [0.046167, 0.953833],\n",
       "        [0.201464, 0.798536],\n",
       "        [0.697174, 0.302826],\n",
       "        [0.924301, 0.075699]]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:5], predictions_probability[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-9a7YVFSDHsX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-9a7YVFSDHsX",
    "outputId": "499ae71d-be0a-4091-af4f-541d453767d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.82      0.84       482\n",
      "         1.0       0.83      0.88      0.85       499\n",
      "\n",
      "    accuracy                           0.85       981\n",
      "   macro avg       0.85      0.85      0.85       981\n",
      "weighted avg       0.85      0.85      0.85       981\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytest,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4b281370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lazypredict\n",
      "  Downloading lazypredict-0.2.12-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: click in c:\\users\\ahzam.imam\\appdata\\local\\anaconda3\\lib\\site-packages (from lazypredict) (8.0.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ahzam.imam\\appdata\\local\\anaconda3\\lib\\site-packages (from lazypredict) (1.3.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\ahzam.imam\\appdata\\local\\anaconda3\\lib\\site-packages (from lazypredict) (2.0.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ahzam.imam\\appdata\\local\\anaconda3\\lib\\site-packages (from lazypredict) (4.65.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\ahzam.imam\\appdata\\local\\anaconda3\\lib\\site-packages (from lazypredict) (1.2.0)\n",
      "Collecting lightgbm (from lazypredict)\n",
      "  Obtaining dependency information for lightgbm from https://files.pythonhosted.org/packages/b3/f8/ee33e36194eb03a76eccf3adac3fba51f0e56fbd20609bb531659d48d3cb/lightgbm-4.1.0-py3-none-win_amd64.whl.metadata\n",
      "  Downloading lightgbm-4.1.0-py3-none-win_amd64.whl.metadata (19 kB)\n",
      "Collecting xgboost (from lazypredict)\n",
      "  Obtaining dependency information for xgboost from https://files.pythonhosted.org/packages/bc/43/242432efc3f60052a4a534dc4926b21e236ab4ec8d4920c593da3f65c65d/xgboost-2.0.2-py3-none-win_amd64.whl.metadata\n",
      "  Downloading xgboost-2.0.2-py3-none-win_amd64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ahzam.imam\\appdata\\local\\anaconda3\\lib\\site-packages (from click->lazypredict) (0.4.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\ahzam.imam\\appdata\\local\\anaconda3\\lib\\site-packages (from lightgbm->lazypredict) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\ahzam.imam\\appdata\\local\\anaconda3\\lib\\site-packages (from lightgbm->lazypredict) (1.11.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ahzam.imam\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas->lazypredict) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ahzam.imam\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas->lazypredict) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ahzam.imam\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas->lazypredict) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ahzam.imam\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn->lazypredict) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ahzam.imam\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->lazypredict) (1.16.0)\n",
      "Downloading lightgbm-4.1.0-py3-none-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.0/1.3 MB 487.6 kB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.1/1.3 MB 655.4 kB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.1/1.3 MB 722.1 kB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.2/1.3 MB 871.5 kB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.3/1.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.4/1.3 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.6/1.3 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.8/1.3 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.1/1.3 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.3/1.3 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 2.6 MB/s eta 0:00:00\n",
      "Downloading xgboost-2.0.2-py3-none-win_amd64.whl (99.8 MB)\n",
      "   ---------------------------------------- 0.0/99.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.9/99.8 MB 27.7 MB/s eta 0:00:04\n",
      "   ---------------------------------------- 1.0/99.8 MB 22.0 MB/s eta 0:00:05\n",
      "    --------------------------------------- 1.5/99.8 MB 11.9 MB/s eta 0:00:09\n",
      "    --------------------------------------- 2.3/99.8 MB 13.5 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 3.5/99.8 MB 15.9 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 3.7/99.8 MB 13.9 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 4.9/99.8 MB 15.5 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 5.3/99.8 MB 15.4 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 6.4/99.8 MB 16.4 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 7.0/99.8 MB 15.9 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 8.2/99.8 MB 16.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 9.2/99.8 MB 17.3 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 10.0/99.8 MB 17.8 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 10.4/99.8 MB 16.8 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 11.8/99.8 MB 19.9 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 13.1/99.8 MB 20.5 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 13.3/99.8 MB 19.8 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 13.3/99.8 MB 19.8 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 13.3/99.8 MB 19.8 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 13.3/99.8 MB 19.8 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 13.3/99.8 MB 19.8 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 13.3/99.8 MB 19.8 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 14.1/99.8 MB 13.4 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 15.6/99.8 MB 14.5 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 16.7/99.8 MB 14.6 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 18.7/99.8 MB 15.2 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 18.9/99.8 MB 14.5 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 18.9/99.8 MB 14.5 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 18.9/99.8 MB 14.5 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 18.9/99.8 MB 14.5 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 18.9/99.8 MB 14.5 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 19.3/99.8 MB 11.1 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 20.6/99.8 MB 11.7 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 20.6/99.8 MB 11.7 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 20.6/99.8 MB 11.7 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 21.2/99.8 MB 10.2 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 21.8/99.8 MB 9.8 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 24.3/99.8 MB 14.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 25.8/99.8 MB 14.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 26.4/99.8 MB 13.9 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 27.4/99.8 MB 13.6 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 28.2/99.8 MB 13.6 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 29.0/99.8 MB 13.6 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 30.3/99.8 MB 18.7 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 31.5/99.8 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 31.5/99.8 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 33.0/99.8 MB 21.8 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 33.6/99.8 MB 21.1 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 33.9/99.8 MB 18.7 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 34.5/99.8 MB 17.7 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.0/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.2/99.8 MB 3.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 36.3/99.8 MB 1.0 MB/s eta 0:01:03\n",
      "   -------------- ------------------------- 36.3/99.8 MB 1.0 MB/s eta 0:01:03\n",
      "   -------------- ------------------------- 36.3/99.8 MB 1.0 MB/s eta 0:01:03\n",
      "   -------------- ------------------------- 36.3/99.8 MB 1.0 MB/s eta 0:01:03\n",
      "   -------------- ------------------------- 36.3/99.8 MB 1.0 MB/s eta 0:01:03\n",
      "   -------------- ------------------------- 36.3/99.8 MB 1.0 MB/s eta 0:01:03\n",
      "   -------------- ------------------------- 36.3/99.8 MB 1.0 MB/s eta 0:01:03\n",
      "   -------------- ------------------------- 36.3/99.8 MB 1.0 MB/s eta 0:01:03\n",
      "   -------------- ------------------------- 36.3/99.8 MB 1.0 MB/s eta 0:01:03\n",
      "   -------------- ------------------------- 36.3/99.8 MB 1.0 MB/s eta 0:01:03\n",
      "   -------------- ------------------------- 36.3/99.8 MB 1.0 MB/s eta 0:01:03\n",
      "   -------------- ------------------------- 36.3/99.8 MB 1.0 MB/s eta 0:01:03\n",
      "   -------------- ------------------------- 36.3/99.8 MB 968.5 kB/s eta 0:01:06\n",
      "   -------------- ------------------------- 36.3/99.8 MB 968.5 kB/s eta 0:01:06\n",
      "   -------------- ------------------------- 36.3/99.8 MB 968.5 kB/s eta 0:01:06\n",
      "   -------------- ------------------------- 36.3/99.8 MB 968.5 kB/s eta 0:01:06\n",
      "   -------------- ------------------------- 36.3/99.8 MB 968.5 kB/s eta 0:01:06\n",
      "   -------------- ------------------------- 36.3/99.8 MB 968.5 kB/s eta 0:01:06\n",
      "   -------------- ------------------------- 36.3/99.8 MB 939.3 kB/s eta 0:01:08\n",
      "   -------------- ------------------------- 36.3/99.8 MB 939.3 kB/s eta 0:01:08\n",
      "   -------------- ------------------------- 36.3/99.8 MB 939.3 kB/s eta 0:01:08\n",
      "   -------------- ------------------------- 36.3/99.8 MB 939.3 kB/s eta 0:01:08\n",
      "   -------------- ------------------------- 36.3/99.8 MB 939.3 kB/s eta 0:01:08\n",
      "   -------------- ------------------------- 36.3/99.8 MB 922.1 kB/s eta 0:01:09\n",
      "   -------------- ------------------------- 36.3/99.8 MB 922.1 kB/s eta 0:01:09\n",
      "   -------------- ------------------------- 36.3/99.8 MB 922.1 kB/s eta 0:01:09\n",
      "   -------------- ------------------------- 36.3/99.8 MB 922.1 kB/s eta 0:01:09\n",
      "   -------------- ------------------------- 36.3/99.8 MB 922.1 kB/s eta 0:01:09\n",
      "   -------------- ------------------------- 36.3/99.8 MB 905.5 kB/s eta 0:01:11\n",
      "   -------------- ------------------------- 36.3/99.8 MB 905.5 kB/s eta 0:01:11\n",
      "   -------------- ------------------------- 36.3/99.8 MB 905.5 kB/s eta 0:01:11\n",
      "   -------------- ------------------------- 36.3/99.8 MB 905.5 kB/s eta 0:01:11\n",
      "   -------------- ------------------------- 36.3/99.8 MB 888.3 kB/s eta 0:01:12\n",
      "   -------------- ------------------------- 36.3/99.8 MB 888.3 kB/s eta 0:01:12\n",
      "   -------------- ------------------------- 36.4/99.8 MB 880.0 kB/s eta 0:01:13\n",
      "   -------------- ------------------------- 36.4/99.8 MB 880.0 kB/s eta 0:01:13\n",
      "   -------------- ------------------------- 36.4/99.8 MB 880.0 kB/s eta 0:01:13\n",
      "   -------------- ------------------------- 36.4/99.8 MB 872.9 kB/s eta 0:01:13\n",
      "   -------------- ------------------------- 36.4/99.8 MB 872.9 kB/s eta 0:01:13\n",
      "   -------------- ------------------------- 36.4/99.8 MB 872.9 kB/s eta 0:01:13\n",
      "   -------------- ------------------------- 36.4/99.8 MB 859.1 kB/s eta 0:01:14\n",
      "   -------------- ------------------------- 36.4/99.8 MB 859.1 kB/s eta 0:01:14\n",
      "   -------------- ------------------------- 36.4/99.8 MB 859.1 kB/s eta 0:01:14\n",
      "   -------------- ------------------------- 36.4/99.8 MB 852.5 kB/s eta 0:01:15\n",
      "   -------------- ------------------------- 36.4/99.8 MB 852.5 kB/s eta 0:01:15\n",
      "   -------------- ------------------------- 36.4/99.8 MB 844.8 kB/s eta 0:01:15\n",
      "   -------------- ------------------------- 36.4/99.8 MB 844.8 kB/s eta 0:01:15\n",
      "   -------------- ------------------------- 36.4/99.8 MB 844.8 kB/s eta 0:01:15\n",
      "   -------------- ------------------------- 36.4/99.8 MB 837.3 kB/s eta 0:01:16\n",
      "   -------------- ------------------------- 36.4/99.8 MB 837.3 kB/s eta 0:01:16\n",
      "   -------------- ------------------------- 36.5/99.8 MB 829.8 kB/s eta 0:01:17\n",
      "   -------------- ------------------------- 36.5/99.8 MB 829.8 kB/s eta 0:01:17\n",
      "   -------------- ------------------------- 36.5/99.8 MB 822.5 kB/s eta 0:01:17\n",
      "   -------------- ------------------------- 36.5/99.8 MB 822.5 kB/s eta 0:01:17\n",
      "   -------------- ------------------------- 36.5/99.8 MB 817.4 kB/s eta 0:01:18\n",
      "   -------------- ------------------------- 36.5/99.8 MB 812.3 kB/s eta 0:01:18\n",
      "   -------------- ------------------------- 36.5/99.8 MB 812.3 kB/s eta 0:01:18\n",
      "   -------------- ------------------------- 36.5/99.8 MB 808.3 kB/s eta 0:01:19\n",
      "   -------------- ------------------------- 36.5/99.8 MB 808.3 kB/s eta 0:01:19\n",
      "   -------------- ------------------------- 36.5/99.8 MB 808.3 kB/s eta 0:01:19\n",
      "   -------------- ------------------------- 36.5/99.8 MB 800.4 kB/s eta 0:01:19\n",
      "   -------------- ------------------------- 36.5/99.8 MB 800.4 kB/s eta 0:01:19\n",
      "   -------------- ------------------------- 37.2/99.8 MB 796.5 kB/s eta 0:01:19\n",
      "   -------------- ------------------------- 37.3/99.8 MB 794.5 kB/s eta 0:01:19\n",
      "   -------------- ------------------------- 37.3/99.8 MB 794.5 kB/s eta 0:01:19\n",
      "   -------------- ------------------------- 37.3/99.8 MB 788.8 kB/s eta 0:01:20\n",
      "   -------------- ------------------------- 37.3/99.8 MB 788.8 kB/s eta 0:01:20\n",
      "   -------------- ------------------------- 37.3/99.8 MB 788.8 kB/s eta 0:01:20\n",
      "   -------------- ------------------------- 37.3/99.8 MB 788.8 kB/s eta 0:01:20\n",
      "   -------------- ------------------------- 37.3/99.8 MB 788.8 kB/s eta 0:01:20\n",
      "   -------------- ------------------------- 37.3/99.8 MB 788.8 kB/s eta 0:01:20\n",
      "   -------------- ------------------------- 37.3/99.8 MB 788.8 kB/s eta 0:01:20\n",
      "   -------------- ------------------------- 37.3/99.8 MB 788.8 kB/s eta 0:01:20\n",
      "   -------------- ------------------------- 37.3/99.8 MB 788.8 kB/s eta 0:01:20\n",
      "   -------------- ------------------------- 37.3/99.8 MB 788.8 kB/s eta 0:01:20\n",
      "   -------------- ------------------------- 37.3/99.8 MB 788.8 kB/s eta 0:01:20\n",
      "   -------------- ------------------------- 37.3/99.8 MB 788.8 kB/s eta 0:01:20\n",
      "   -------------- ------------------------- 37.3/99.8 MB 788.8 kB/s eta 0:01:20\n",
      "   -------------- ------------------------- 37.3/99.8 MB 788.8 kB/s eta 0:01:20\n",
      "   -------------- ------------------------- 37.3/99.8 MB 788.8 kB/s eta 0:01:20\n",
      "   -------------- ------------------------- 37.3/99.8 MB 788.8 kB/s eta 0:01:20\n",
      "   -------------- ------------------------- 37.3/99.8 MB 745.7 kB/s eta 0:01:24\n",
      "   -------------- ------------------------- 37.3/99.8 MB 745.7 kB/s eta 0:01:24\n",
      "   -------------- ------------------------- 37.3/99.8 MB 745.7 kB/s eta 0:01:24\n",
      "   -------------- ------------------------- 37.3/99.8 MB 745.7 kB/s eta 0:01:24\n",
      "   -------------- ------------------------- 37.3/99.8 MB 745.7 kB/s eta 0:01:24\n",
      "   -------------- ------------------------- 37.3/99.8 MB 745.7 kB/s eta 0:01:24\n",
      "   -------------- ------------------------- 37.3/99.8 MB 745.7 kB/s eta 0:01:24\n",
      "   -------------- ------------------------- 37.3/99.8 MB 745.7 kB/s eta 0:01:24\n",
      "   -------------- ------------------------- 37.3/99.8 MB 745.7 kB/s eta 0:01:24\n",
      "   -------------- ------------------------- 37.3/99.8 MB 721.0 kB/s eta 0:01:27\n",
      "   -------------- ------------------------- 37.3/99.8 MB 721.0 kB/s eta 0:01:27\n",
      "   -------------- ------------------------- 37.3/99.8 MB 721.0 kB/s eta 0:01:27\n",
      "   -------------- ------------------------- 37.3/99.8 MB 721.0 kB/s eta 0:01:27\n",
      "   -------------- ------------------------- 37.3/99.8 MB 721.0 kB/s eta 0:01:27\n",
      "   -------------- ------------------------- 37.3/99.8 MB 721.0 kB/s eta 0:01:27\n",
      "   -------------- ------------------------- 37.3/99.8 MB 707.0 kB/s eta 0:01:29\n",
      "   -------------- ------------------------- 37.3/99.8 MB 707.0 kB/s eta 0:01:29\n",
      "   -------------- ------------------------- 37.3/99.8 MB 701.7 kB/s eta 0:01:29\n",
      "   -------------- ------------------------- 37.3/99.8 MB 701.7 kB/s eta 0:01:29\n",
      "   -------------- ------------------------- 37.3/99.8 MB 701.7 kB/s eta 0:01:29\n",
      "   -------------- ------------------------- 37.4/99.8 MB 696.5 kB/s eta 0:01:30\n",
      "   -------------- ------------------------- 37.4/99.8 MB 692.8 kB/s eta 0:01:31\n",
      "   -------------- ------------------------- 37.4/99.8 MB 692.8 kB/s eta 0:01:31\n",
      "   -------------- ------------------------- 37.4/99.8 MB 689.2 kB/s eta 0:01:31\n",
      "   --------------- ------------------------ 37.4/99.8 MB 686.2 kB/s eta 0:01:31\n",
      "   --------------- ------------------------ 37.4/99.8 MB 684.1 kB/s eta 0:01:32\n",
      "   --------------- ------------------------ 37.5/99.8 MB 682.0 kB/s eta 0:01:32\n",
      "   --------------- ------------------------ 37.5/99.8 MB 680.5 kB/s eta 0:01:32\n",
      "   --------------- ------------------------ 37.5/99.8 MB 677.7 kB/s eta 0:01:32\n",
      "   --------------- ------------------------ 37.5/99.8 MB 677.7 kB/s eta 0:01:32\n",
      "   --------------- ------------------------ 37.6/99.8 MB 674.3 kB/s eta 0:01:33\n",
      "   --------------- ------------------------ 37.6/99.8 MB 674.3 kB/s eta 0:01:33\n",
      "   --------------- ------------------------ 37.6/99.8 MB 670.1 kB/s eta 0:01:33\n",
      "   --------------- ------------------------ 37.6/99.8 MB 670.1 kB/s eta 0:01:33\n",
      "   --------------- ------------------------ 37.7/99.8 MB 666.7 kB/s eta 0:01:34\n",
      "   --------------- ------------------------ 37.7/99.8 MB 665.4 kB/s eta 0:01:34\n",
      "   --------------- ------------------------ 37.7/99.8 MB 663.3 kB/s eta 0:01:34\n",
      "   --------------- ------------------------ 37.8/99.8 MB 660.7 kB/s eta 0:01:34\n",
      "   --------------- ------------------------ 37.8/99.8 MB 658.7 kB/s eta 0:01:35\n",
      "   --------------- ------------------------ 37.9/99.8 MB 656.0 kB/s eta 0:01:35\n",
      "   --------------- ------------------------ 37.9/99.8 MB 655.4 kB/s eta 0:01:35\n",
      "   --------------- ------------------------ 37.9/99.8 MB 652.7 kB/s eta 0:01:35\n",
      "   --------------- ------------------------ 38.0/99.8 MB 650.2 kB/s eta 0:01:36\n",
      "   --------------- ------------------------ 38.0/99.8 MB 648.9 kB/s eta 0:01:36\n",
      "   --------------- ------------------------ 38.0/99.8 MB 647.0 kB/s eta 0:01:36\n",
      "   --------------- ------------------------ 38.1/99.8 MB 645.0 kB/s eta 0:01:36\n",
      "   --------------- ------------------------ 38.1/99.8 MB 644.4 kB/s eta 0:01:36\n",
      "   --------------- ------------------------ 38.2/99.8 MB 641.2 kB/s eta 0:01:37\n",
      "   --------------- ------------------------ 38.2/99.8 MB 640.6 kB/s eta 0:01:37\n",
      "   --------------- ------------------------ 38.3/99.8 MB 637.5 kB/s eta 0:01:37\n",
      "   --------------- ------------------------ 38.3/99.8 MB 635.7 kB/s eta 0:01:37\n",
      "   --------------- ------------------------ 38.3/99.8 MB 633.8 kB/s eta 0:01:37\n",
      "   --------------- ------------------------ 38.4/99.8 MB 633.8 kB/s eta 0:01:37\n",
      "   --------------- ------------------------ 38.5/99.8 MB 632.0 kB/s eta 0:01:37\n",
      "   --------------- ------------------------ 38.5/99.8 MB 630.1 kB/s eta 0:01:38\n",
      "   --------------- ------------------------ 38.6/99.8 MB 628.3 kB/s eta 0:01:38\n",
      "   --------------- ------------------------ 38.6/99.8 MB 627.1 kB/s eta 0:01:38\n",
      "   --------------- ------------------------ 38.7/99.8 MB 624.7 kB/s eta 0:01:38\n",
      "   --------------- ------------------------ 38.8/99.8 MB 623.0 kB/s eta 0:01:38\n",
      "   --------------- ------------------------ 38.8/99.8 MB 621.8 kB/s eta 0:01:39\n",
      "   --------------- ------------------------ 38.9/99.8 MB 620.0 kB/s eta 0:01:39\n",
      "   --------------- ------------------------ 38.9/99.8 MB 618.8 kB/s eta 0:01:39\n",
      "   --------------- ------------------------ 39.0/99.8 MB 616.5 kB/s eta 0:01:39\n",
      "   --------------- ------------------------ 39.0/99.8 MB 614.8 kB/s eta 0:01:39\n",
      "   --------------- ------------------------ 39.1/99.8 MB 613.0 kB/s eta 0:01:39\n",
      "   --------------- ------------------------ 39.2/99.8 MB 611.3 kB/s eta 0:01:40\n",
      "   --------------- ------------------------ 39.3/99.8 MB 611.3 kB/s eta 0:01:39\n",
      "   --------------- ------------------------ 39.3/99.8 MB 610.1 kB/s eta 0:01:40\n",
      "   --------------- ------------------------ 39.4/99.8 MB 607.9 kB/s eta 0:01:40\n",
      "   --------------- ------------------------ 39.5/99.8 MB 605.6 kB/s eta 0:01:40\n",
      "   --------------- ------------------------ 39.6/99.8 MB 604.5 kB/s eta 0:01:40\n",
      "   --------------- ------------------------ 39.7/99.8 MB 604.0 kB/s eta 0:01:40\n",
      "   --------------- ------------------------ 39.7/99.8 MB 602.3 kB/s eta 0:01:40\n",
      "   --------------- ------------------------ 39.8/99.8 MB 600.7 kB/s eta 0:01:40\n",
      "   ---------------- ----------------------- 39.9/99.8 MB 599.6 kB/s eta 0:01:40\n",
      "   ---------------- ----------------------- 40.0/99.8 MB 597.9 kB/s eta 0:01:40\n",
      "   ---------------- ----------------------- 40.1/99.8 MB 596.3 kB/s eta 0:01:41\n",
      "   ---------------- ----------------------- 40.2/99.8 MB 595.2 kB/s eta 0:01:41\n",
      "   ---------------- ----------------------- 40.3/99.8 MB 593.5 kB/s eta 0:01:41\n",
      "   ---------------- ----------------------- 40.4/99.8 MB 591.9 kB/s eta 0:01:41\n",
      "   ---------------- ----------------------- 40.5/99.8 MB 590.4 kB/s eta 0:01:41\n",
      "   ---------------- ----------------------- 40.6/99.8 MB 588.8 kB/s eta 0:01:41\n",
      "   ---------------- ----------------------- 40.7/99.8 MB 587.7 kB/s eta 0:01:41\n",
      "   ---------------- ----------------------- 40.9/99.8 MB 585.6 kB/s eta 0:01:41\n",
      "   ---------------- ----------------------- 41.0/99.8 MB 584.6 kB/s eta 0:01:41\n",
      "   ---------------- ----------------------- 41.1/99.8 MB 582.5 kB/s eta 0:01:41\n",
      "   ---------------- ----------------------- 41.2/99.8 MB 581.5 kB/s eta 0:01:41\n",
      "   ---------------- ----------------------- 41.3/99.8 MB 579.9 kB/s eta 0:01:41\n",
      "   ---------------- ----------------------- 41.5/99.8 MB 578.4 kB/s eta 0:01:41\n",
      "   ---------------- ----------------------- 41.6/99.8 MB 576.8 kB/s eta 0:01:41\n",
      "   ---------------- ----------------------- 41.7/99.8 MB 575.3 kB/s eta 0:01:41\n",
      "   ---------------- ----------------------- 41.9/99.8 MB 576.8 kB/s eta 0:01:41\n",
      "   ---------------- ----------------------- 42.0/99.8 MB 574.8 kB/s eta 0:01:41\n",
      "   ---------------- ----------------------- 42.1/99.8 MB 573.3 kB/s eta 0:01:41\n",
      "   ---------------- ----------------------- 42.3/99.8 MB 571.8 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 42.4/99.8 MB 570.8 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 42.6/99.8 MB 568.8 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 42.8/99.8 MB 567.3 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 42.9/99.8 MB 565.4 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 43.0/99.8 MB 565.4 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 43.1/99.8 MB 564.9 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 43.3/99.8 MB 562.9 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 43.5/99.8 MB 562.0 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 43.6/99.8 MB 560.0 kB/s eta 0:01:41\n",
      "   ----------------- ---------------------- 43.9/99.8 MB 560.0 kB/s eta 0:01:40\n",
      "   ----------------- ---------------------- 44.0/99.8 MB 559.1 kB/s eta 0:01:40\n",
      "   ----------------- ---------------------- 44.2/99.8 MB 557.7 kB/s eta 0:01:40\n",
      "   ----------------- ---------------------- 44.3/99.8 MB 557.2 kB/s eta 0:01:40\n",
      "   ----------------- ---------------------- 44.6/99.8 MB 556.2 kB/s eta 0:01:40\n",
      "   ----------------- ---------------------- 44.7/99.8 MB 554.8 kB/s eta 0:01:40\n",
      "   ------------------ --------------------- 44.9/99.8 MB 553.4 kB/s eta 0:01:40\n",
      "   ------------------ --------------------- 45.1/99.8 MB 553.4 kB/s eta 0:01:39\n",
      "   ------------------ --------------------- 45.4/99.8 MB 552.0 kB/s eta 0:01:39\n",
      "   ------------------ --------------------- 45.5/99.8 MB 636.9 kB/s eta 0:01:26\n",
      "   ------------------ --------------------- 45.7/99.8 MB 635.6 kB/s eta 0:01:26\n",
      "   ------------------ --------------------- 45.9/99.8 MB 633.8 kB/s eta 0:01:25\n",
      "   ------------------ --------------------- 46.2/99.8 MB 631.9 kB/s eta 0:01:25\n",
      "   ------------------ --------------------- 46.4/99.8 MB 630.1 kB/s eta 0:01:25\n",
      "   ------------------ --------------------- 46.4/99.8 MB 629.5 kB/s eta 0:01:25\n",
      "   ------------------ --------------------- 46.8/99.8 MB 1.6 MB/s eta 0:00:34\n",
      "   ------------------ --------------------- 47.0/99.8 MB 1.6 MB/s eta 0:00:34\n",
      "   ------------------ --------------------- 47.2/99.8 MB 1.5 MB/s eta 0:00:34\n",
      "   ------------------ --------------------- 47.3/99.8 MB 1.5 MB/s eta 0:00:35\n",
      "   ------------------- -------------------- 47.7/99.8 MB 2.2 MB/s eta 0:00:24\n",
      "   ------------------- -------------------- 47.8/99.8 MB 2.4 MB/s eta 0:00:22\n",
      "   ------------------- -------------------- 48.1/99.8 MB 2.5 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 48.3/99.8 MB 2.7 MB/s eta 0:00:20\n",
      "   ------------------- -------------------- 48.6/99.8 MB 2.9 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 48.9/99.8 MB 3.1 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 49.1/99.8 MB 3.1 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 49.5/99.8 MB 3.4 MB/s eta 0:00:15\n",
      "   ------------------- -------------------- 49.6/99.8 MB 3.4 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 50.0/99.8 MB 3.6 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 50.2/99.8 MB 3.7 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 50.5/99.8 MB 3.8 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 50.9/99.8 MB 4.0 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 51.0/99.8 MB 4.0 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 51.2/99.8 MB 4.1 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 51.7/99.8 MB 4.4 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 51.8/99.8 MB 4.3 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 52.2/99.8 MB 4.4 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 52.5/99.8 MB 4.6 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 52.9/99.8 MB 4.7 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 53.2/99.8 MB 4.9 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 53.6/99.8 MB 5.0 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 54.0/99.8 MB 5.2 MB/s eta 0:00:09\n",
      "   --------------------- ------------------ 54.4/99.8 MB 5.4 MB/s eta 0:00:09\n",
      "   --------------------- ------------------ 54.6/99.8 MB 5.5 MB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 55.2/99.8 MB 5.7 MB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 55.5/99.8 MB 5.8 MB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 55.9/99.8 MB 5.9 MB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 55.9/99.8 MB 5.9 MB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 55.9/99.8 MB 5.9 MB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 57.1/99.8 MB 6.3 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 57.3/99.8 MB 6.5 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 57.7/99.8 MB 6.6 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 58.0/99.8 MB 6.5 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 58.2/99.8 MB 6.6 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 58.6/99.8 MB 6.7 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 58.8/99.8 MB 6.7 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 59.1/99.8 MB 6.9 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 59.5/99.8 MB 6.8 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 59.8/99.8 MB 6.7 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 60.0/99.8 MB 6.8 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 60.4/99.8 MB 6.9 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 60.6/99.8 MB 7.0 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 61.0/99.8 MB 7.0 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 61.4/99.8 MB 7.1 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 61.5/99.8 MB 7.1 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 62.0/99.8 MB 7.2 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 62.2/99.8 MB 7.1 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 62.6/99.8 MB 7.2 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 63.0/99.8 MB 7.1 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 63.4/99.8 MB 7.1 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 63.6/99.8 MB 7.1 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 64.0/99.8 MB 7.0 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 64.2/99.8 MB 7.0 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 64.6/99.8 MB 7.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 65.0/99.8 MB 7.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 65.4/99.8 MB 7.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 65.7/99.8 MB 7.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 66.2/99.8 MB 7.5 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 66.4/99.8 MB 7.3 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 66.7/99.8 MB 7.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 67.2/99.8 MB 6.9 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 67.4/99.8 MB 6.8 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 67.6/99.8 MB 6.9 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 68.2/99.8 MB 7.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 68.5/99.8 MB 7.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 68.8/99.8 MB 7.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 69.3/99.8 MB 7.1 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 69.5/99.8 MB 7.1 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 69.9/99.8 MB 7.2 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 70.3/99.8 MB 7.2 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 70.6/99.8 MB 7.2 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 70.9/99.8 MB 7.3 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 71.4/99.8 MB 7.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 71.8/99.8 MB 7.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 72.1/99.8 MB 7.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 72.6/99.8 MB 7.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 72.9/99.8 MB 7.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 73.3/99.8 MB 7.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 73.7/99.8 MB 7.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 74.0/99.8 MB 7.5 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 74.4/99.8 MB 7.5 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 74.8/99.8 MB 7.5 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 75.2/99.8 MB 7.5 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 75.6/99.8 MB 7.6 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 76.0/99.8 MB 7.7 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 76.5/99.8 MB 7.7 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 76.9/99.8 MB 7.9 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 77.3/99.8 MB 7.7 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 77.7/99.8 MB 7.9 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 78.0/99.8 MB 8.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 78.4/99.8 MB 8.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 78.8/99.8 MB 8.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 79.3/99.8 MB 8.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 79.6/99.8 MB 8.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 79.9/99.8 MB 8.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 80.3/99.8 MB 8.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 80.6/99.8 MB 8.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 80.9/99.8 MB 8.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 81.5/99.8 MB 8.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 81.6/99.8 MB 8.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 82.1/99.8 MB 8.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 82.3/99.8 MB 8.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 82.8/99.8 MB 8.1 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 83.1/99.8 MB 8.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 83.5/99.8 MB 8.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 83.8/99.8 MB 8.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 84.4/99.8 MB 8.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 84.7/99.8 MB 8.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 84.9/99.8 MB 8.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 84.9/99.8 MB 8.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 85.9/99.8 MB 8.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 86.2/99.8 MB 8.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 86.6/99.8 MB 8.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 86.7/99.8 MB 8.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 87.0/99.8 MB 7.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 87.4/99.8 MB 7.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 87.7/99.8 MB 7.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 88.0/99.8 MB 7.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 88.1/99.8 MB 7.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 88.4/99.8 MB 7.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 88.7/99.8 MB 7.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 89.0/99.8 MB 7.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 89.4/99.8 MB 7.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 89.6/99.8 MB 7.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 89.8/99.8 MB 7.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 90.2/99.8 MB 7.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 90.4/99.8 MB 7.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 90.8/99.8 MB 6.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 91.1/99.8 MB 7.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 91.3/99.8 MB 6.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 91.6/99.8 MB 6.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 92.0/99.8 MB 6.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 92.2/99.8 MB 6.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 92.6/99.8 MB 6.8 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 92.9/99.8 MB 6.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 93.2/99.8 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 93.3/99.8 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 93.7/99.8 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 94.1/99.8 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 94.1/99.8 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 94.3/99.8 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 94.6/99.8 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.0/99.8 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.3/99.8 MB 6.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.6/99.8 MB 6.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.8/99.8 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.3/99.8 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.5/99.8 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.8/99.8 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 97.1/99.8 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 97.2/99.8 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  97.7/99.8 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.0/99.8 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.0/99.8 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.2/99.8 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.7/99.8 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.8/99.8 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.3/99.8 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.6/99.8 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 99.8/99.8 MB 4.6 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost, lightgbm, lazypredict\n",
      "Successfully installed lazypredict-0.2.12 lightgbm-4.1.0 xgboost-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2226019d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 28/29 [01:00<00:01,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4407, number of negative: 4420\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12239\n",
      "[LightGBM] [Info] Number of data points in the train set: 8827, number of used features: 48\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499264 -> initscore=-0.002946\n",
      "[LightGBM] [Info] Start training from score -0.002946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [01:00<00:00,  2.09s/it]\n"
     ]
    }
   ],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(featuress,labelss,test_size=0.1)\n",
    "clf = LazyClassifier(verbose = 0, ignore_warnings=True, custom_metric=None)\n",
    "model, predictions = clf.fit(xtrain,xtest,ytrain,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "04a7e94a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "ExtraTreesClassifier               0.98               0.98     0.98      0.98   \n",
      "XGBClassifier                      0.97               0.97     0.97      0.97   \n",
      "LGBMClassifier                     0.96               0.96     0.96      0.96   \n",
      "RandomForestClassifier             0.96               0.96     0.96      0.96   \n",
      "BaggingClassifier                  0.95               0.95     0.95      0.95   \n",
      "LabelPropagation                   0.92               0.92     0.92      0.92   \n",
      "LabelSpreading                     0.92               0.92     0.92      0.92   \n",
      "ExtraTreeClassifier                0.92               0.92     0.92      0.92   \n",
      "DecisionTreeClassifier             0.91               0.91     0.91      0.91   \n",
      "AdaBoostClassifier                 0.91               0.91     0.91      0.91   \n",
      "SVC                                0.90               0.89     0.89      0.89   \n",
      "KNeighborsClassifier               0.87               0.87     0.87      0.87   \n",
      "LinearSVC                          0.84               0.84     0.84      0.84   \n",
      "SGDClassifier                      0.84               0.84     0.84      0.84   \n",
      "LogisticRegression                 0.83               0.83     0.83      0.83   \n",
      "CalibratedClassifierCV             0.83               0.83     0.83      0.83   \n",
      "NuSVC                              0.83               0.83     0.83      0.83   \n",
      "RidgeClassifierCV                  0.78               0.78     0.78      0.78   \n",
      "RidgeClassifier                    0.78               0.78     0.78      0.78   \n",
      "LinearDiscriminantAnalysis         0.78               0.78     0.78      0.78   \n",
      "PassiveAggressiveClassifier        0.78               0.77     0.77      0.77   \n",
      "Perceptron                         0.75               0.75     0.75      0.75   \n",
      "QuadraticDiscriminantAnalysis      0.74               0.74     0.74      0.73   \n",
      "GaussianNB                         0.73               0.72     0.72      0.71   \n",
      "NearestCentroid                    0.69               0.69     0.69      0.69   \n",
      "BernoulliNB                        0.68               0.68     0.68      0.68   \n",
      "DummyClassifier                    0.48               0.50     0.50      0.31   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "ExtraTreesClassifier                 2.43  \n",
      "XGBClassifier                        1.13  \n",
      "LGBMClassifier                       0.76  \n",
      "RandomForestClassifier              21.80  \n",
      "BaggingClassifier                   17.14  \n",
      "LabelPropagation                    11.10  \n",
      "LabelSpreading                      10.26  \n",
      "ExtraTreeClassifier                  0.07  \n",
      "DecisionTreeClassifier               2.26  \n",
      "AdaBoostClassifier                  11.22  \n",
      "SVC                                  4.40  \n",
      "KNeighborsClassifier                 0.19  \n",
      "LinearSVC                            2.35  \n",
      "SGDClassifier                        0.21  \n",
      "LogisticRegression                   0.31  \n",
      "CalibratedClassifierCV               1.06  \n",
      "NuSVC                                8.74  \n",
      "RidgeClassifierCV                    0.30  \n",
      "RidgeClassifier                      0.10  \n",
      "LinearDiscriminantAnalysis           0.53  \n",
      "PassiveAggressiveClassifier          0.09  \n",
      "Perceptron                           0.07  \n",
      "QuadraticDiscriminantAnalysis        0.74  \n",
      "GaussianNB                           0.06  \n",
      "NearestCentroid                      0.25  \n",
      "BernoulliNB                          0.08  \n",
      "DummyClassifier                      0.06  \n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "62edba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(featuress, labelss, test_size=0.1)\n",
    "\n",
    "# Instantiate and train ExtraTreesClassifier\n",
    "extra_trees_model = ExtraTreesClassifier()\n",
    "extra_trees_model.fit(xtrain, ytrain)\n",
    "\n",
    "# Save the ExtraTreesClassifier model\n",
    "with open('Fraud-Detection.pkl', 'wb') as model_file:\n",
    "    pickle.dump(extra_trees_model, model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "106f2e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n",
      "Non-fraudulent\n",
      "Non-fraudulent\n",
      "Fraudulent\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the saved ExtraTreesClassifier model\n",
    "with open('Fraud-Detection.pkl', 'rb') as model_file:\n",
    "    extra_trees_model = pickle.load(model_file)\n",
    "\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = extra_trees_model.predict(xtest)\n",
    "\n",
    "# Interpret the predictions\n",
    "for prediction in predictions:\n",
    "    if prediction == 0:\n",
    "        print(\"Non-fraudulent\")\n",
    "    else:\n",
    "        print(\"Fraudulent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "825e0c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "model_file_path = 'Fraud-Detection.pkl'\n",
    "\n",
    "# Load the trained model\n",
    "with open(model_file_path, 'rb') as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "\n",
    "# Sample random data\n",
    "random_data = pd.DataFrame({\n",
    "    'phy_same': [2.96],\n",
    "    'phy_count': [1.6],\n",
    "    'period': [1.44],\n",
    "    'periodadmt': [1.0],\n",
    "    'age': [80.24],\n",
    "    'alife': [1.0],\n",
    "    'provider_InscClaimAmtReimbursed_mean': [4185.6],\n",
    "    'provider_DeductibleAmtPaid_mean': [213.6],\n",
    "    'provider_NoOfMonths_PartACov_mean': [12],\n",
    "    'provider_NoOfMonths_PartBCov_mean': [12],\n",
    "    'provider_IPAnnualReimbursementAmt_mean': [17606],\n",
    "    'provider_IPAnnualDeductibleAmt_mean': [897.12],\n",
    "    'provider_OPAnnualReimbursementAmt_mean': [2615.2],\n",
    "    'provider_OPAnnualDeductibleAmt_mean': [463.92],\n",
    "    'provider_InscClaimAmtReimbursed_std': [10796.09114],\n",
    "    'provider_DeductibleAmtPaid_std': [436.0091742],\n",
    "    'provider_NoOfMonths_PartACov_std': [0],\n",
    "    'provider_NoOfMonths_PartBCov_std': [0],\n",
    "    'provider_IPAnnualReimbursementAmt_std': [38895.47746],\n",
    "    'provider_IPAnnualDeductibleAmt_std': [1332.50567],\n",
    "    'provider_OPAnnualReimbursementAmt_std': [2974.176525],\n",
    "    'provider_OPAnnualDeductibleAmt_std': [635.7823212],\n",
    "    'banel_InscClaimAmtReimbursed_mean': [3284.621746],\n",
    "    'banel_DeductibleAmtPaid_mean': [184.4771962],\n",
    "    'banel_NoOfMonths_PartACov_mean': [12],\n",
    "    'banel_NoOfMonths_PartBCov_mean': [12],\n",
    "    'banel_IPAnnualReimbursementAmt_mean': [17606],\n",
    "    'banel_IPAnnualDeductibleAmt_mean': [897.12],\n",
    "    'banel_OPAnnualReimbursementAmt_mean': [2615.2],\n",
    "    'banel_OPAnnualDeductibleAmt_mean': [463.92],\n",
    "    'banel_InscClaimAmtReimbursed_std': [4972.142171],\n",
    "    'banel_DeductibleAmtPaid_std': [216.0606318],\n",
    "    'banel_NoOfMonths_PartACov_std': [0],\n",
    "    'banel_NoOfMonths_PartBCov_std': [0],\n",
    "    'banel_IPAnnualReimbursementAmt_std': [0],\n",
    "    'banel_IPAnnualDeductibleAmt_std': [0],\n",
    "    'banel_OPAnnualReimbursementAmt_std': [0],\n",
    "    'banel_OPAnnualDeductibleAmt_std': [0],\n",
    "    'diag1_InscClaimAmtReimbursed_mean': [2441.642986],\n",
    "    'diag1_DeductibleAmtPaid_mean': [168.8697495],\n",
    "    'diag1_NoOfMonths_PartACov_mean': [11.94507249],\n",
    "    'diag1_NoOfMonths_PartBCov_mean': [11.9656874],\n",
    "    'diag1_IPAnnualReimbursementAmt_mean': [7579.093008],\n",
    "    'diag1_IPAnnualDeductibleAmt_mean': [717.6620268],\n",
    "    'diag1_OPAnnualReimbursementAmt_mean': [1804.331094],\n",
    "    'diag1_OPAnnualDeductibleAmt_mean': [543.0450837],\n",
    "    'diag1_InscClaimAmtReimbursed_std': [3482.06631],\n",
    "    'diag1_DeductibleAmtPaid_std': [161.353027],\n",
    "    'diag1_NoOfMonths_PartACov_std': [0.569944911],\n",
    "    'diag1_NoOfMonths_PartBCov_std': [0.42419154],\n",
    "    'diag1_IPAnnualReimbursementAmt_std': [12941.55235],\n",
    "    'diag1_IPAnnualDeductibleAmt_std': [1205.297144],\n",
    "    'diag1_OPAnnualReimbursementAmt_std': [2450.076771],\n",
    "    'diag1_OPAnnualDeductibleAmt_std': [661.5066723]\n",
    "})\n",
    "\n",
    "# Standardize the input data using the same scaler used during training\n",
    "scaler = StandardScaler()\n",
    "random_data_stand = scaler.fit_transform(random_data)\n",
    "\n",
    "# Make predictions\n",
    "prediction = model.predict(random_data_stand)\n",
    "\n",
    "# Display the prediction\n",
    "print(\"Predicted Label:\", prediction[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "09fa926a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   phy_same  phy_count  period  periodadmt   age  alife  \\\n",
      "2      2.82       1.60    1.43        0.00 73.52   0.99   \n",
      "\n",
      "   provider_InscClaimAmtReimbursed_mean  provider_DeductibleAmtPaid_mean  \\\n",
      "2                                350.13                             2.08   \n",
      "\n",
      "   provider_NoOfMonths_PartACov_mean  provider_NoOfMonths_PartBCov_mean  ...  \\\n",
      "2                              11.87                              11.96  ...   \n",
      "\n",
      "   diag1_OPAnnualReimbursementAmt_mean  diag1_OPAnnualDeductibleAmt_mean  \\\n",
      "2                              2430.02                            694.25   \n",
      "\n",
      "   diag1_InscClaimAmtReimbursed_std  diag1_DeductibleAmtPaid_std  \\\n",
      "2                           1536.29                       113.09   \n",
      "\n",
      "   diag1_NoOfMonths_PartACov_std  diag1_NoOfMonths_PartBCov_std  \\\n",
      "2                           0.67                           0.58   \n",
      "\n",
      "   diag1_IPAnnualReimbursementAmt_std  diag1_IPAnnualDeductibleAmt_std  \\\n",
      "2                            11016.52                          1111.59   \n",
      "\n",
      "   diag1_OPAnnualReimbursementAmt_std  diag1_OPAnnualDeductibleAmt_std  \n",
      "2                             2972.38                           808.14  \n",
      "\n",
      "[1 rows x 54 columns]\n",
      "Predicted Label: Fraud\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "# Replace this with your actual file path\n",
    "model_file_path = 'Fraud-Detection.pkl'\n",
    "preprocessed_data_path = 'Pre Processed Dataset.csv'\n",
    "\n",
    "# Load the trained model\n",
    "with open(model_file_path, 'rb') as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "\n",
    "preprocessed_data = pd.read_csv(preprocessed_data_path)\n",
    "random_data = preprocessed_data.iloc[[2], 2:]\n",
    "\n",
    "print(random_data)\n",
    "\n",
    "# Standardize the input data using the same scaler used during training\n",
    "scaler = StandardScaler()\n",
    "random_data_stand = scaler.fit_transform(random_data)\n",
    "\n",
    "# Make predictions\n",
    "prediction = model.predict(random_data_stand)\n",
    "\n",
    "if prediction[0] == 1:\n",
    "    result = \"Fraud\"\n",
    "else:\n",
    "    result = \"Not Fraud\"\n",
    "\n",
    "# Display the result\n",
    "print(\"Predicted Label:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bfaa082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1: Provider - PRV51001, Predicted Label - Potential Not Fraud\n",
      "Row 2: Provider - PRV51003, Predicted Label - Potential Fraud\n",
      "Row 3: Provider - PRV51004, Predicted Label - Potential Not Fraud\n",
      "Row 4: Provider - PRV51005, Predicted Label - Potential Fraud\n",
      "Row 5: Provider - PRV51007, Predicted Label - Potential Not Fraud\n",
      "Row 6: Provider - PRV51008, Predicted Label - Potential Fraud\n",
      "Row 7: Provider - PRV51011, Predicted Label - Potential Fraud\n",
      "Row 8: Provider - PRV51012, Predicted Label - Potential Not Fraud\n",
      "Row 9: Provider - PRV51013, Predicted Label - Potential Fraud\n",
      "Row 10: Provider - PRV51014, Predicted Label - Potential Not Fraud\n",
      "Row 11: Provider - PRV51015, Predicted Label - Potential Not Fraud\n",
      "Row 12: Provider - PRV51016, Predicted Label - Potential Not Fraud\n",
      "Row 13: Provider - PRV51017, Predicted Label - Potential Fraud\n",
      "Row 14: Provider - PRV51021, Predicted Label - Potential Fraud\n",
      "Row 15: Provider - PRV51023, Predicted Label - Potential Not Fraud\n",
      "Row 16: Provider - PRV51024, Predicted Label - Potential Fraud\n",
      "Row 17: Provider - PRV51025, Predicted Label - Potential Fraud\n",
      "Row 18: Provider - PRV51026, Predicted Label - Potential Not Fraud\n",
      "Row 19: Provider - PRV51027, Predicted Label - Potential Not Fraud\n",
      "Row 20: Provider - PRV51029, Predicted Label - Potential Not Fraud\n",
      "Row 21: Provider - PRV51030, Predicted Label - Potential Fraud\n",
      "Row 22: Provider - PRV51031, Predicted Label - Potential Fraud\n",
      "Row 23: Provider - PRV51032, Predicted Label - Potential Not Fraud\n",
      "Row 24: Provider - PRV51035, Predicted Label - Potential Not Fraud\n",
      "Row 25: Provider - PRV51036, Predicted Label - Potential Not Fraud\n",
      "Row 26: Provider - PRV51037, Predicted Label - Potential Fraud\n",
      "Row 27: Provider - PRV51038, Predicted Label - Potential Fraud\n",
      "Row 28: Provider - PRV51040, Predicted Label - Potential Fraud\n",
      "Row 29: Provider - PRV51041, Predicted Label - Potential Not Fraud\n",
      "Row 30: Provider - PRV51042, Predicted Label - Potential Not Fraud\n",
      "Row 31: Provider - PRV51043, Predicted Label - Potential Not Fraud\n",
      "Row 32: Provider - PRV51044, Predicted Label - Potential Not Fraud\n",
      "Row 33: Provider - PRV51045, Predicted Label - Potential Not Fraud\n",
      "Row 34: Provider - PRV51046, Predicted Label - Potential Not Fraud\n",
      "Row 35: Provider - PRV51047, Predicted Label - Potential Not Fraud\n",
      "Row 36: Provider - PRV51048, Predicted Label - Potential Not Fraud\n",
      "Row 37: Provider - PRV51049, Predicted Label - Potential Not Fraud\n",
      "Row 38: Provider - PRV51052, Predicted Label - Potential Fraud\n",
      "Row 39: Provider - PRV51053, Predicted Label - Potential Not Fraud\n",
      "Row 40: Provider - PRV51054, Predicted Label - Potential Not Fraud\n",
      "Row 41: Provider - PRV51055, Predicted Label - Potential Not Fraud\n",
      "Row 42: Provider - PRV51056, Predicted Label - Potential Not Fraud\n",
      "Row 43: Provider - PRV51057, Predicted Label - Potential Not Fraud\n",
      "Row 44: Provider - PRV51058, Predicted Label - Potential Fraud\n",
      "Row 45: Provider - PRV51059, Predicted Label - Potential Fraud\n",
      "Row 46: Provider - PRV51060, Predicted Label - Potential Not Fraud\n",
      "Row 47: Provider - PRV51061, Predicted Label - Potential Fraud\n",
      "Row 48: Provider - PRV51062, Predicted Label - Potential Not Fraud\n",
      "Row 49: Provider - PRV51063, Predicted Label - Potential Not Fraud\n",
      "Row 50: Provider - PRV51064, Predicted Label - Potential Fraud\n",
      "Row 51: Provider - PRV51065, Predicted Label - Potential Fraud\n",
      "Row 52: Provider - PRV51066, Predicted Label - Potential Not Fraud\n",
      "Row 53: Provider - PRV51067, Predicted Label - Potential Fraud\n",
      "Row 54: Provider - PRV51068, Predicted Label - Potential Fraud\n",
      "Row 55: Provider - PRV51070, Predicted Label - Potential Fraud\n",
      "Row 56: Provider - PRV51071, Predicted Label - Potential Not Fraud\n",
      "Row 57: Provider - PRV51072, Predicted Label - Potential Fraud\n",
      "Row 58: Provider - PRV51074, Predicted Label - Potential Fraud\n",
      "Row 59: Provider - PRV51075, Predicted Label - Potential Not Fraud\n",
      "Row 60: Provider - PRV51076, Predicted Label - Potential Not Fraud\n",
      "Row 61: Provider - PRV51077, Predicted Label - Potential Fraud\n",
      "Row 62: Provider - PRV51078, Predicted Label - Potential Not Fraud\n",
      "Row 63: Provider - PRV51081, Predicted Label - Potential Not Fraud\n",
      "Row 64: Provider - PRV51082, Predicted Label - Potential Not Fraud\n",
      "Row 65: Provider - PRV51083, Predicted Label - Potential Not Fraud\n",
      "Row 66: Provider - PRV51084, Predicted Label - Potential Fraud\n",
      "Row 67: Provider - PRV51086, Predicted Label - Potential Fraud\n",
      "Row 68: Provider - PRV51089, Predicted Label - Potential Fraud\n",
      "Row 69: Provider - PRV51090, Predicted Label - Potential Not Fraud\n",
      "Row 70: Provider - PRV51091, Predicted Label - Potential Not Fraud\n",
      "Row 71: Provider - PRV51093, Predicted Label - Potential Not Fraud\n",
      "Row 72: Provider - PRV51096, Predicted Label - Potential Not Fraud\n",
      "Row 73: Provider - PRV51097, Predicted Label - Potential Not Fraud\n",
      "Row 74: Provider - PRV51098, Predicted Label - Potential Not Fraud\n",
      "Row 75: Provider - PRV51100, Predicted Label - Potential Not Fraud\n",
      "Row 76: Provider - PRV51101, Predicted Label - Potential Not Fraud\n",
      "Row 77: Provider - PRV51102, Predicted Label - Potential Fraud\n",
      "Row 78: Provider - PRV51103, Predicted Label - Potential Not Fraud\n",
      "Row 79: Provider - PRV51104, Predicted Label - Potential Not Fraud\n",
      "Row 80: Provider - PRV51105, Predicted Label - Potential Not Fraud\n",
      "Row 81: Provider - PRV51108, Predicted Label - Potential Fraud\n",
      "Row 82: Provider - PRV51109, Predicted Label - Potential Not Fraud\n",
      "Row 83: Provider - PRV51110, Predicted Label - Potential Not Fraud\n",
      "Row 84: Provider - PRV51111, Predicted Label - Potential Not Fraud\n",
      "Row 85: Provider - PRV51112, Predicted Label - Potential Not Fraud\n",
      "Row 86: Provider - PRV51113, Predicted Label - Potential Not Fraud\n",
      "Row 87: Provider - PRV51114, Predicted Label - Potential Fraud\n",
      "Row 88: Provider - PRV51115, Predicted Label - Potential Not Fraud\n",
      "Row 89: Provider - PRV51117, Predicted Label - Potential Not Fraud\n",
      "Row 90: Provider - PRV51118, Predicted Label - Potential Fraud\n",
      "Row 91: Provider - PRV51119, Predicted Label - Potential Fraud\n",
      "Row 92: Provider - PRV51120, Predicted Label - Potential Not Fraud\n",
      "Row 93: Provider - PRV51121, Predicted Label - Potential Not Fraud\n",
      "Row 94: Provider - PRV51122, Predicted Label - Potential Not Fraud\n",
      "Row 95: Provider - PRV51123, Predicted Label - Potential Not Fraud\n",
      "Row 96: Provider - PRV51125, Predicted Label - Potential Fraud\n",
      "Row 97: Provider - PRV51126, Predicted Label - Potential Fraud\n",
      "Row 98: Provider - PRV51128, Predicted Label - Potential Fraud\n",
      "Row 99: Provider - PRV51129, Predicted Label - Potential Not Fraud\n",
      "Row 100: Provider - PRV51130, Predicted Label - Potential Not Fraud\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "# Replace this with your actual file path\n",
    "model_file_path = 'Fraud-Detection.pkl'\n",
    "preprocessed_data_path = 'Pre Processed Dataset.csv'\n",
    "\n",
    "# Load the trained model\n",
    "with open(model_file_path, 'rb') as model_file:\n",
    "    model = pickle.load(model_file)\n",
    "\n",
    "# Read the pre-processed data, excluding the first column (Provider)\n",
    "preprocessed_data = pd.read_csv(preprocessed_data_path)\n",
    "\n",
    "features=preprocessed_data.iloc[:,2:]\n",
    "labels=preprocessed_data.iloc[:,1]\n",
    "\n",
    "# Standardize the input data using the same scaler used during training\n",
    "scaler = StandardScaler()\n",
    "preprocessed_data_stand = scaler.fit_transform(preprocessed_data.iloc[:, 2:101])\n",
    "\n",
    "# Make predictions for the first 100 rows\n",
    "for i in range(100):\n",
    "    # Extract the i-th row\n",
    "    random_data_stand = preprocessed_data_stand[i].reshape(1, -1)\n",
    "\n",
    "    # Make predictions\n",
    "    prediction = model.predict(random_data_stand)\n",
    "\n",
    "    # Interpret the prediction\n",
    "    if prediction[0] == 1:\n",
    "        result = \"Potential Fraud\"\n",
    "    else:\n",
    "        result = \"Potential Not Fraud\"\n",
    "\n",
    "    # Extract the Provider for the i-th row\n",
    "    provider = preprocessed_data.iloc[i, 0]\n",
    "\n",
    "    # Display the result and Provider for the i-th row\n",
    "    print(f\"Row {i+1}: Provider - {provider}, Predicted Label - {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8884ae3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessed_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(preprocessed_data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessed_data' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf72660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 215.440442,
   "end_time": "2023-10-24T17:51:42.876045",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-24T17:48:07.435603",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
